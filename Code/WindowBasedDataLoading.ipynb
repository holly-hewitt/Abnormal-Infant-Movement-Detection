{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.enable_eager_execution(tf.ConfigProto(log_device_placement=True))\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accelData from pickle\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# open accelData pickle\n",
    "import pickle\n",
    "with open('drive/MyDrive/Pickles/accelData.pickle', 'rb') as handle:\n",
    "    accelData = pickle.load(handle)\n",
    "\n",
    "# Remove time column from accelData Measurements\n",
    "accelData['Measurements with Time'] = accelData['Measurements']\n",
    "\n",
    "for i in range(len(accelData['Measurements'])):\n",
    "    accelData['Measurements'][i] = accelData['Measurements'][i].iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window generator function to save memory\n",
    "def window_generator_old(data, labels, windowSize, stride, batch_size=128):\n",
    "    while True:  # Loop forever so the generator never terminates\n",
    "        batch_features = []\n",
    "        batch_labels = []\n",
    "        for i in np.random.permutation(len(data)):\n",
    "            print(f'Processing trial {i} of {len(data)}')\n",
    "            windows = createWindows(data[i], windowSize, stride)\n",
    "            for window in windows:\n",
    "                # Normalize the window\n",
    "                window_normalized = (window - np.mean(window)) / np.std(window)\n",
    "                batch_features.append(window_normalized)\n",
    "                batch_labels.append(labels[i])\n",
    "\n",
    "                if len(batch_features) == batch_size:\n",
    "                    print('Yielding batch')\n",
    "                    print(np.array(batch_features).shape)\n",
    "                    # Yield the batch data\n",
    "                    yield np.array(batch_features), np.array(batch_labels)\n",
    "                    batch_features = []\n",
    "                    batch_labels = []\n",
    "\n",
    "def window_generator(data, labels, windowSize, stride, batch_size=128):\n",
    "    while True:  # Loop forever so the generator never terminates\n",
    "        batch_features = []\n",
    "        batch_labels = []\n",
    "        batch_windows = []\n",
    "        for i in np.random.permutation(len(data)):\n",
    "            batch_windows.append(data[i])\n",
    "            # Normalize the window\n",
    "            #window_normalized = (window - np.mean(window, axis=0)) / np.std(window, axis=0)\n",
    "            #batch_features.append(window_normalized)\n",
    "            batch_labels.append(labels[i])\n",
    "        \n",
    "\n",
    "\n",
    "            if len(batch_windows) == batch_size:\n",
    "                #batch_windows_array = np.array(batch_windows)\n",
    "                #batch_features= vectorized_normalization(batch_windows_array)\n",
    "                # Yield the batch data\n",
    "                yield np.array(batch_windows), np.array(batch_labels)\n",
    "                batch_features = []\n",
    "                batch_labels = []\n",
    "\n",
    "\n",
    "def vectorized_normalization(windows):\n",
    "    # Compute means and standard deviations for each window\n",
    "    means = windows.mean(axis=1, keepdims=True)\n",
    "    stds = windows.std(axis=1, keepdims=True)\n",
    "\n",
    "    # Normalize\n",
    "    normalized_windows = (windows - means) / stds\n",
    "\n",
    "    return normalized_windows\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPooling1D, Normalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# class weight import\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    oldmodel = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.5),\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model = Sequential([\n",
    "        Normalization(input_shape=input_shape),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),  # Reduced filters\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),  # Removed one Conv1D and one Dropout layer for simplicity\n",
    "        Dense(64, activation='relu'),  # Reduced the size of the dense layer\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  run_eagerly=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createWindows(data, windowSize, stride):\n",
    "    windows = []\n",
    "    for i in range(0, len(data) - windowSize, stride):\n",
    "        windows.append(data[i:i+windowSize])\n",
    "    return windows\n",
    "    \n",
    "def batchWindowClassification(trials, labels, windowSize, stride, batch_size=100, verbose=False):\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(trials, labels, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    if verbose:\n",
    "        # plot histogram showing distribution of labels\n",
    "        plt.hist(y_train)\n",
    "        # title\n",
    "        plt.title('Distribution of labels in training set')\n",
    "        plt.show()\n",
    "\n",
    "        plt.hist(y_test)\n",
    "        # title\n",
    "        plt.title('Distribution of labels in testing set')\n",
    "        plt.show()  \n",
    "\n",
    "\n",
    "        print(y_train)\n",
    "        print(y_test)\n",
    "\n",
    "    unique_labels = np.unique(np.concatenate((y_train, y_test)))\n",
    "    num_classes = len(unique_labels)\n",
    "    if verbose:\n",
    "        print(f\"Number of unique classes: {num_classes}\")\n",
    "\n",
    "    # Prepare labels for training\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(np.concatenate((y_train, y_test)))\n",
    "\n",
    "    # Split X_train into windows\n",
    "    trainWindows = []\n",
    "    trainWindowLabels = []\n",
    "    for i in range(len(X_train)):\n",
    "        windows = createWindows(X_train[i], windowSize, stride)\n",
    "        trainWindows.extend(windows)\n",
    "        labelArray = np.full(len(windows), y_train[i])\n",
    "        # Convert labelArray to a list\n",
    "        trainWindowLabels.extend(labelArray.tolist())\n",
    "\n",
    "    valWindows = []\n",
    "    valWindowLabels = []\n",
    "    for i in range(len(X_val)):\n",
    "        windows = createWindows(X_val[i], windowSize, stride)\n",
    "        valWindows.extend(windows)\n",
    "        labelArray = np.full(len(windows), y_val[i])\n",
    "        # Convert labelArray to a list\n",
    "        valWindowLabels.extend(labelArray.tolist())\n",
    "    \n",
    "    print(len(trainWindows))\n",
    "\n",
    "    # encode trainWindowLabels\n",
    "    encoded_trainWindowLabels = encoder.transform(trainWindowLabels)\n",
    "    encoded_trainWindowLabels = to_categorical(encoded_trainWindowLabels, num_classes=num_classes)\n",
    "\n",
    "    # encode valWindowLabels\n",
    "    encoded_valWindowLabels = encoder.transform(valWindowLabels)\n",
    "    encoded_valWindowLabels = to_categorical(encoded_valWindowLabels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "    # Creating generators for training and validation\n",
    "    train_gen = window_generator(trainWindows, encoded_trainWindowLabels, windowSize, stride, batch_size=batch_size)\n",
    "    val_gen = window_generator(valWindows, encoded_valWindowLabels, windowSize, stride, batch_size=batch_size)\n",
    "\n",
    "    # Assuming `y_train` contains the original labels\n",
    "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "    model = build_model((windowSize, 16), 3)\n",
    "\n",
    "    # Adapting the Normalization layer with a batch of data from the training set\n",
    "    for X_batch, _ in window_generator(trainWindows, encoded_trainWindowLabels, windowSize, stride, batch_size=128):\n",
    "        model.layers[0].adapt(X_batch)  # Adapt the normalization layer\n",
    "        break \n",
    "\n",
    "    # Instantiate the callback\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "    \n",
    "    # Assume your model is named 'model' and is already defined\n",
    "    model.fit(train_gen, \n",
    "        #callbacks=[reduce_lr], \n",
    "        #validation_data=val_gen,\n",
    "        #steps_per_epoch=max(1, len(trainWindows) // batch_size), \n",
    "        steps_per_epoch=100,\n",
    "        epochs=10, \n",
    "        verbose=1) #, class_weight=class_weights_dict)    \n",
    "\n",
    "    # Split X_train into windows\n",
    "    testWindows = []\n",
    "    testWindowLabels = []\n",
    "    for i in range(len(X_test)):\n",
    "        windows = createWindows(X_test[i], windowSize, stride)\n",
    "        testWindows.extend(windows)\n",
    "        labelArray = np.full(len(windows), y_test[i])\n",
    "        # Convert labelArray to a list\n",
    "        testWindowLabels.extend(labelArray.tolist())\n",
    "\n",
    "    # encode testWindowLabels\n",
    "    encoded_testWindowLabels = encoder.transform(testWindowLabels)\n",
    "    encoded_testWindowLabels = to_categorical(encoded_testWindowLabels, num_classes=num_classes)\n",
    "\n",
    "    # Creating generators for testing\n",
    "    test_gen = window_generator(testWindows, encoded_testWindowLabels, windowSize, stride, batch_size=batch_size)\n",
    "    test_steps = max(1, len(testWindows) // batch_size)  # Ensure at least 1 step\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(test_gen, steps= test_steps, verbose=1)\n",
    "\n",
    "    # Print out predictions\n",
    "    predictions = model.predict(test_gen, steps=test_steps, verbose=1)\n",
    "\n",
    "    # transform predictions to labels\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    predictions = encoder.inverse_transform(predictions)\n",
    "    # plot histogram showing distribution of predictions\n",
    "    plt.hist(predictions)\n",
    "    # title\n",
    "    plt.title('Distribution of predictions')\n",
    "    plt.show()\n",
    "\n",
    "    # print distribution of predictions\n",
    "    print(np.unique(predictions, return_counts=True))\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Loss: {loss}')\n",
    "\n",
    "    return loss, accuracy, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windowSize: 100, stride: 50.0\n",
      "226197\n",
      "WARNING:tensorflow:AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x000001B6F8A40E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x000001B6F8A40E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001B723A4E678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001B723A4E678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 0.8570 - accuracy: 0.6285\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 0.6048 - accuracy: 0.6852\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 15s 149ms/step - loss: 0.5862 - accuracy: 0.6892\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 16s 158ms/step - loss: 0.5852 - accuracy: 0.7015\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 0.5764 - accuracy: 0.7038\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.5576 - accuracy: 0.7160\n",
      "Epoch 7/10\n",
      " 35/100 [=========>....................] - ETA: 12s - loss: 0.5593 - accuracy: 0.7091"
     ]
    }
   ],
   "source": [
    "# Test batchWindowClassification with different windowSizes, and strides of windowSizes/2 \n",
    "# Make batch size inversely proportional to windowSize\n",
    "# Make verbose=False\n",
    "\n",
    "windowSizes = [100, 1000, 5000, 10000]\n",
    "strides = [50, 500, 2500, 5000]\n",
    "allResults = []\n",
    "\n",
    "for windowSize in windowSizes:\n",
    "    \n",
    "    print(f'windowSize: {windowSize}, stride: {windowSize/2}')\n",
    "    loss, accuracy, model = batchWindowClassification(accelData['Measurements'], accelData['Abnormal'], windowSize, int(windowSize/2), batch_size=100, verbose=False)\n",
    "    \n",
    "    # save results in variable\n",
    "    results = {\n",
    "        'windowSize': windowSize,\n",
    "        'stride': stride,\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "        'model': model\n",
    "    }\n",
    "    allResults.append(results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
