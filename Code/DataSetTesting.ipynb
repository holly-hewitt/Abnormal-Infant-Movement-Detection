{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to allow GPU access\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.enable_eager_execution(tf.ConfigProto(log_device_placement=False)) \n",
    "tf.test.gpu_device_name()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Masking\n",
    "from sklearn.model_selection import KFold\n",
    "# import early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dataset: X_smoothed_mean_norm\n",
      "Working on fold: 1\n",
      "Fitting model\n",
      "Train on 102 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "102/102 [==============================] - 3s 31ms/sample - loss: 33.0345 - acc: 0.6569 - val_loss: 59.8339 - val_acc: 0.2308\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 18.6116 - acc: 0.7059 - val_loss: 13.5074 - val_acc: 0.5769\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 4.7788 - acc: 0.8824 - val_loss: 10.5273 - val_acc: 0.7308\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 1.3504 - acc: 0.9412 - val_loss: 12.3153 - val_acc: 0.6923\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 0.4632 - acc: 0.9804 - val_loss: 19.5235 - val_acc: 0.7308\n",
      "Predicting test set\n",
      "Working on fold: 2\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 30ms/sample - loss: 21.8473 - acc: 0.5340 - val_loss: 7.6023 - val_acc: 0.6923\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 3.3547 - acc: 0.8447 - val_loss: 10.6831 - val_acc: 0.6154\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 1.3227 - acc: 0.8835 - val_loss: 16.7016 - val_acc: 0.6154\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 1.0121 - acc: 0.9612 - val_loss: 14.1021 - val_acc: 0.5769\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 0.0188 - acc: 0.9903 - val_loss: 14.9573 - val_acc: 0.6154\n",
      "Predicting test set\n",
      "Working on fold: 3\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 34ms/sample - loss: 68.4057 - acc: 0.5049 - val_loss: 49.6241 - val_acc: 0.4231\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 31.3763 - acc: 0.7476 - val_loss: 31.1614 - val_acc: 0.5769\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 8.5323 - acc: 0.9320 - val_loss: 35.0139 - val_acc: 0.7308\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.4301 - acc: 0.9515 - val_loss: 22.1213 - val_acc: 0.6154\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 1.1844 - acc: 0.9806 - val_loss: 30.6422 - val_acc: 0.7692\n",
      "Predicting test set\n",
      "Working on fold: 4\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 31ms/sample - loss: 17.4353 - acc: 0.5340 - val_loss: 10.3106 - val_acc: 0.7308\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 3.8354 - acc: 0.8252 - val_loss: 13.4865 - val_acc: 0.5385\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.0127 - acc: 0.8835 - val_loss: 13.5649 - val_acc: 0.6923\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 2.6422 - acc: 0.9029 - val_loss: 12.5321 - val_acc: 0.7308\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.0421 - acc: 0.8835 - val_loss: 20.1936 - val_acc: 0.7692\n",
      "Predicting test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fold: 5\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 32ms/sample - loss: 30.9918 - acc: 0.5728 - val_loss: 42.6383 - val_acc: 0.7308\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 22.4983 - acc: 0.8350 - val_loss: 76.2052 - val_acc: 0.5769\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 11.1115 - acc: 0.8738 - val_loss: 66.6028 - val_acc: 0.7308\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 3.4770 - acc: 0.9515 - val_loss: 85.6624 - val_acc: 0.6923\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 3.5695 - acc: 0.9320 - val_loss: 78.4828 - val_acc: 0.6923\n",
      "Predicting test set\n",
      "Working on dataset: X_smoothed_median_norm\n",
      "Working on fold: 1\n",
      "Fitting model\n",
      "Train on 102 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "102/102 [==============================] - 3s 32ms/sample - loss: 38.6772 - acc: 0.5196 - val_loss: 14.8563 - val_acc: 0.7692\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 8.2303 - acc: 0.7647 - val_loss: 5.0356 - val_acc: 0.6923\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 0.8206 - acc: 0.8725 - val_loss: 1.9721 - val_acc: 0.7692\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 0.1878 - acc: 0.9510 - val_loss: 3.6294 - val_acc: 0.7692\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 3s 26ms/sample - loss: 0.0138 - acc: 0.9902 - val_loss: 2.3734 - val_acc: 0.7308\n",
      "Predicting test set\n",
      "Working on fold: 2\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 31ms/sample - loss: 16.2522 - acc: 0.5922 - val_loss: 12.7691 - val_acc: 0.6538\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 3.1886 - acc: 0.7476 - val_loss: 10.1638 - val_acc: 0.3846\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 1.6181 - acc: 0.8738 - val_loss: 8.1522 - val_acc: 0.6538\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 0.2678 - acc: 0.9612 - val_loss: 7.5215 - val_acc: 0.5769\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 0.0042 - acc: 1.0000 - val_loss: 8.4693 - val_acc: 0.6923\n",
      "Predicting test set\n",
      "Working on fold: 3\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 31ms/sample - loss: 29.3632 - acc: 0.5340 - val_loss: 14.2125 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 14.1682 - acc: 0.7184 - val_loss: 24.5815 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 2.6007 - acc: 0.9126 - val_loss: 11.6490 - val_acc: 0.6154\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 0.6975 - acc: 0.9709 - val_loss: 13.0815 - val_acc: 0.5385\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 0.1012 - acc: 0.9903 - val_loss: 13.9121 - val_acc: 0.6923\n",
      "Predicting test set\n",
      "Working on fold: 4\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 33ms/sample - loss: 26.9943 - acc: 0.5534 - val_loss: 12.4725 - val_acc: 0.7308\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 4.9396 - acc: 0.7573 - val_loss: 19.8271 - val_acc: 0.7692\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.2093 - acc: 0.9029 - val_loss: 15.3663 - val_acc: 0.6923\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 0.4660 - acc: 0.9612 - val_loss: 22.9255 - val_acc: 0.7308\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 0.5637 - acc: 0.9515 - val_loss: 20.2067 - val_acc: 0.7308\n",
      "Predicting test set\n",
      "Working on fold: 5\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 31ms/sample - loss: 16.2838 - acc: 0.5146 - val_loss: 13.8876 - val_acc: 0.6923\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 3.8853 - acc: 0.8350 - val_loss: 9.2251 - val_acc: 0.5769\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 1.3158 - acc: 0.9223 - val_loss: 11.8832 - val_acc: 0.5769\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 0.2017 - acc: 0.9612 - val_loss: 14.1866 - val_acc: 0.6154\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 0.1132 - acc: 0.9903 - val_loss: 19.3544 - val_acc: 0.6923\n",
      "Predicting test set\n",
      "{'X_smoothed_mean_norm': {'Accuracy': (0.646780303030303, 0.09973348898243625), 'Sensitivity': (0.25665445665445663, 0.20892954310299036), 'False Positive Rate': (0.10536689549961861, 0.08887055485462426), 'Specificity': (0.8946331045003815, 0.08887055485462426), 'Precision': (0.4442424242424242, 0.23422184236678187)}, 'X_smoothed_median_norm': {'Accuracy': (0.6096590909090909, 0.08332343260744607), 'Sensitivity': (0.2796703296703297, 0.20223134745596413), 'False Positive Rate': (0.15133790999237223, 0.10233053478672907), 'Specificity': (0.8486620900076278, 0.10233053478672907), 'Precision': (0.47555555555555556, 0.08703624901140507)}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters to be recorded, along with standard deviation:\n",
    "# Accuracy \n",
    "# Sensitivity(Recall) \n",
    "# False Positive Rate \n",
    "# Specificity \n",
    "# Precision\n",
    "\n",
    "def create_model(filters=32, kernel_size=3, dropout_rate=0.5):    \n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0., input_shape=(19301, 16)))  # Adjust the input_shape to match your dataset\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu', input_shape=(19301, 16)))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))  # Reduced the number of neurons in the dense layer\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_bin = np.argmax(y_pred, axis=1)\n",
    "    y_true_bin = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    tn = np.sum((y_true_bin == 0) & (y_pred_bin == 0))\n",
    "    fp = np.sum((y_true_bin == 0) & (y_pred_bin != 0))\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    return specificity\n",
    "\n",
    "# Create a nested dictionary to store the results\n",
    "dataset_results = {'X_smoothed_mean_norm': {}, 'X_smoothed_median_norm': {}}\n",
    "\n",
    " # Load in dataset from pickle\n",
    "with open('Pickles/abnormal_encoded.pickle', 'rb') as handle:\n",
    "    abnormal_encoded = pickle.load(handle)\n",
    "\n",
    "def train_and_evaluate():\n",
    "\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Find best dataset to train and test model on\n",
    "    dataset_names = ['X_smoothed_mean_norm', 'X_smoothed_median_norm']   \n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "\n",
    "         # Load in dataset from pickle\n",
    "        with open(f'Pickles/{dataset_name}.pickle', 'rb') as handle:\n",
    "            dataset = pickle.load(handle)\n",
    "        \n",
    "        dataset = np.array(dataset)\n",
    "\n",
    "        print(f'Working on dataset: {dataset_name}')\n",
    "\n",
    "        accuracies = []\n",
    "        sensitivities = []\n",
    "        false_positive_rates = []\n",
    "        specificities = []\n",
    "        precisions = []\n",
    "\n",
    "        fold = 1\n",
    "\n",
    "        for train_index, test_index in outer_cv.split(dataset):\n",
    "\n",
    "            # Print current progress\n",
    "            print(f'Working on fold: {fold}')\n",
    "            fold += 1\n",
    "            \n",
    "            X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "            Y_train, Y_test = abnormal_encoded[train_index], abnormal_encoded[test_index]\n",
    "            \n",
    "            model = create_model()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            \n",
    "            #Fit the model\n",
    "            print('Fitting model')\n",
    "            model.fit(X_train, Y_train, epochs=5, batch_size=4, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "            # Predict the test set\n",
    "            print('Predicting test set')\n",
    "            Y_pred = model.predict(X_test)\n",
    "\n",
    "            Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "            Y_test_classes = np.argmax(Y_test, axis=1)\n",
    "            \n",
    "            # Calulate accuracy, sensitivity, false positive rate, specificity and precision\n",
    "            accuracies.append(accuracy_score(Y_test_classes, Y_pred_classes))\n",
    "            sensitivities.append(recall_score(Y_test_classes, Y_pred_classes))\n",
    "            false_positive_rates.append(1 - specificity_score(Y_test, Y_pred))\n",
    "            specificities.append(specificity_score(Y_test, Y_pred))\n",
    "            precisions.append(precision_score(Y_test_classes, Y_pred_classes))\n",
    "\n",
    "\n",
    "\n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        avg_sensitivity = np.mean(sensitivities)\n",
    "        avg_false_positive_rate = np.mean(false_positive_rates)\n",
    "        avg_specificity = np.mean(specificities)\n",
    "        avg_precision = np.mean(precisions)\n",
    "\n",
    "        std_accuracy = np.std(accuracies)\n",
    "        std_sensitivity = np.std(sensitivities)\n",
    "        std_false_positive_rate = np.std(false_positive_rates)\n",
    "        std_specificity = np.std(specificities)\n",
    "        std_precision = np.std(precisions)\n",
    "\n",
    "        dataset_results[dataset_name]['Accuracy'] = (avg_accuracy, std_accuracy)\n",
    "        dataset_results[dataset_name]['Sensitivity'] = (avg_sensitivity, std_sensitivity)\n",
    "        dataset_results[dataset_name]['False Positive Rate'] = (avg_false_positive_rate, std_false_positive_rate)\n",
    "        dataset_results[dataset_name]['Specificity'] = (avg_specificity, std_specificity)\n",
    "        dataset_results[dataset_name]['Precision'] = (avg_precision, std_precision)\n",
    "\n",
    "        # Delete dataset to free up memory\n",
    "        del dataset\n",
    "        del Y_pred\n",
    "\n",
    "train_and_evaluate()\n",
    "print(dataset_results)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: X_smoothed_mean_norm\n",
      "Accuracy: 0.646780303030303 +/- 0.09973348898243625\n",
      "Sensitivity: 0.25665445665445663 +/- 0.20892954310299036\n",
      "False Positive Rate: 0.10536689549961861 +/- 0.08887055485462426\n",
      "Specificity: 0.8946331045003815 +/- 0.08887055485462426\n",
      "Precision: 0.4442424242424242 +/- 0.23422184236678187\n",
      "\n",
      "\n",
      "Dataset: X_smoothed_median_norm\n",
      "Accuracy: 0.6096590909090909 +/- 0.08332343260744607\n",
      "Sensitivity: 0.2796703296703297 +/- 0.20223134745596413\n",
      "False Positive Rate: 0.15133790999237223 +/- 0.10233053478672907\n",
      "Specificity: 0.8486620900076278 +/- 0.10233053478672907\n",
      "Precision: 0.47555555555555556 +/- 0.08703624901140507\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print dataset results in a nice way:\n",
    "for dataset_name, results in dataset_results.items():\n",
    "    print(f'Dataset: {dataset_name}')\n",
    "    for metric, (avg, std) in results.items():\n",
    "        print(f'{metric}: {avg} +/- {std}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dataset: X_smoothed_mean_norm_month\n",
      "Working on fold: 1\n",
      "Fitting model\n",
      "Train on 102 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "102/102 [==============================] - 9s 85ms/sample - loss: 18.5635 - acc: 0.5588 - val_loss: 5.4278 - val_acc: 0.7692\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 3s 30ms/sample - loss: 3.3363 - acc: 0.8039 - val_loss: 14.0761 - val_acc: 0.7692\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 3s 28ms/sample - loss: 0.6824 - acc: 0.9118 - val_loss: 10.3251 - val_acc: 0.7692\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 3s 28ms/sample - loss: 0.9546 - acc: 0.9216 - val_loss: 5.8729 - val_acc: 0.7692\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 3s 28ms/sample - loss: 0.1600 - acc: 0.9804 - val_loss: 11.4707 - val_acc: 0.7692\n",
      "Predicting test set\n",
      "Working on fold: 2\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 4s 42ms/sample - loss: 39.4155 - acc: 0.4757 - val_loss: 37.1786 - val_acc: 0.3462\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 8.3315 - acc: 0.7864 - val_loss: 12.0778 - val_acc: 0.4615\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 5.4547 - acc: 0.8058 - val_loss: 10.5414 - val_acc: 0.5769\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 0.7617 - acc: 0.9806 - val_loss: 10.4381 - val_acc: 0.5385\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 30ms/sample - loss: 0.0320 - acc: 0.9903 - val_loss: 10.8047 - val_acc: 0.5385\n",
      "Predicting test set\n",
      "Working on fold: 3\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 32ms/sample - loss: 69.2986 - acc: 0.5631 - val_loss: 80.1647 - val_acc: 0.6154\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 19.1810 - acc: 0.7282 - val_loss: 88.5105 - val_acc: 0.5385\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 22.6305 - acc: 0.8641 - val_loss: 145.7286 - val_acc: 0.6923\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 16.0318 - acc: 0.8544 - val_loss: 131.1212 - val_acc: 0.6154\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 29ms/sample - loss: 14.0214 - acc: 0.9029 - val_loss: 114.3915 - val_acc: 0.6538\n",
      "Predicting test set\n",
      "Working on fold: 4\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 32ms/sample - loss: 35.4261 - acc: 0.4563 - val_loss: 37.9533 - val_acc: 0.7308\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 11.6764 - acc: 0.7670 - val_loss: 14.9017 - val_acc: 0.7308\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.3118 - acc: 0.8835 - val_loss: 18.5206 - val_acc: 0.7692\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 0.6797 - acc: 0.9515 - val_loss: 15.0407 - val_acc: 0.6154\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 1.9569e-06 - acc: 1.0000 - val_loss: 16.9308 - val_acc: 0.4615\n",
      "Predicting test set\n",
      "Working on fold: 5\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 31ms/sample - loss: 34.7812 - acc: 0.5631 - val_loss: 39.2554 - val_acc: 0.6154\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 10.3821 - acc: 0.7282 - val_loss: 17.0874 - val_acc: 0.6154\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.0112 - acc: 0.9029 - val_loss: 11.1518 - val_acc: 0.6538\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 0.3554 - acc: 0.9612 - val_loss: 13.8683 - val_acc: 0.6538\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 0.1262 - acc: 0.9903 - val_loss: 18.7626 - val_acc: 0.5769\n",
      "Predicting test set\n",
      "Working on dataset: X_smoothed_median_norm_month\n",
      "Working on fold: 1\n",
      "Fitting model\n",
      "Train on 102 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "102/102 [==============================] - 3s 32ms/sample - loss: 29.8433 - acc: 0.6176 - val_loss: 3.2040 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 3s 28ms/sample - loss: 1.6007 - acc: 0.7745 - val_loss: 9.1674 - val_acc: 0.7692\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 3s 28ms/sample - loss: 1.1238 - acc: 0.8824 - val_loss: 3.9056 - val_acc: 0.5769\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 3s 28ms/sample - loss: 0.7404 - acc: 0.8824 - val_loss: 8.1516 - val_acc: 0.7692\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 3s 28ms/sample - loss: 0.3733 - acc: 0.9314 - val_loss: 4.3113 - val_acc: 0.6154\n",
      "Predicting test set\n",
      "Working on fold: 2\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 33ms/sample - loss: 22.9765 - acc: 0.5437 - val_loss: 17.2972 - val_acc: 0.3846\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 4.9319 - acc: 0.8058 - val_loss: 52.2357 - val_acc: 0.6923\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 3.6733 - acc: 0.8932 - val_loss: 27.4598 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 7.0049 - acc: 0.8252 - val_loss: 82.1599 - val_acc: 0.6923\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.2627 - acc: 0.9515 - val_loss: 20.4641 - val_acc: 0.5769\n",
      "Predicting test set\n",
      "Working on fold: 3\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 32ms/sample - loss: 68.4655 - acc: 0.5340 - val_loss: 22.5533 - val_acc: 0.4615\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 29ms/sample - loss: 7.0639 - acc: 0.7573 - val_loss: 36.1337 - val_acc: 0.6154\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 4.2243 - acc: 0.8835 - val_loss: 25.4710 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 1.4519 - acc: 0.9417 - val_loss: 33.8012 - val_acc: 0.6154\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 2.4108 - acc: 0.9029 - val_loss: 32.9278 - val_acc: 0.5769\n",
      "Predicting test set\n",
      "Working on fold: 4\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 4s 35ms/sample - loss: 30.3364 - acc: 0.5728 - val_loss: 30.8750 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 17.7319 - acc: 0.6990 - val_loss: 20.6888 - val_acc: 0.6154\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 5.7678 - acc: 0.8155 - val_loss: 17.9938 - val_acc: 0.6538\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 0.6739 - acc: 0.9320 - val_loss: 18.9406 - val_acc: 0.6538\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 0.2432 - acc: 0.9806 - val_loss: 21.8664 - val_acc: 0.6923\n",
      "Predicting test set\n",
      "Working on fold: 5\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 32ms/sample - loss: 26.9990 - acc: 0.6019 - val_loss: 11.6172 - val_acc: 0.6538\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.7891 - acc: 0.8350 - val_loss: 4.7571 - val_acc: 0.6154\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 1.0961 - acc: 0.8641 - val_loss: 8.6062 - val_acc: 0.6538\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 26ms/sample - loss: 1.1423 - acc: 0.9029 - val_loss: 8.8416 - val_acc: 0.6154\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 0.4026 - acc: 0.9709 - val_loss: 8.4272 - val_acc: 0.6154\n",
      "Predicting test set\n",
      "{'X_smoothed_mean_norm_month': {'Accuracy': (0.696780303030303, 0.09978095269929742), 'Sensitivity': (0.6442111633599048, 0.07147403266931529), 'False Positive Rate': (0.1610282227307399, 0.08355097822644932), 'Specificity': (0.8389717772692601, 0.08355097822644932), 'Precision': (0.6796386528957743, 0.06602168250829586)}, 'X_smoothed_median_norm_month': {'Accuracy': (0.602840909090909, 0.07257174421116831), 'Sensitivity': (0.5478065649072514, 0.028490875610738826), 'False Positive Rate': (0.26574218154080853, 0.1448530873374119), 'Specificity': (0.7342578184591915, 0.1448530873374119), 'Precision': (0.5717232183021658, 0.05136835587495588)}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters to be recorded, along with standard deviation:\n",
    "# Accuracy \n",
    "# Sensitivity(Recall) \n",
    "# False Positive Rate \n",
    "# Specificity \n",
    "# Precision\n",
    "\n",
    "def create_model(filters=32, kernel_size=3, dropout_rate=0.5):    \n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0., input_shape=(19301, 17)))  # Adjust the input_shape to match your dataset\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu', input_shape=(19301, 17)))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))  # Reduced the number of neurons in the dense layer\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_bin = np.argmax(y_pred, axis=1)\n",
    "    y_true_bin = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    tn = np.sum((y_true_bin == 0) & (y_pred_bin == 0))\n",
    "    fp = np.sum((y_true_bin == 0) & (y_pred_bin != 0))\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    return specificity\n",
    "\n",
    "# Create a nested dictionary to store the results\n",
    "dataset_results = {'X_smoothed_mean_norm_month': {}, 'X_smoothed_median_norm_month': {}}\n",
    "\n",
    " # Load in dataset from pickle\n",
    "with open('Pickles/abnormal_encoded.pickle', 'rb') as handle:\n",
    "    abnormal_encoded = pickle.load(handle)\n",
    "\n",
    "def train_and_evaluate():\n",
    "\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Find best dataset to train and test model on\n",
    "    dataset_names = ['X_smoothed_mean_norm_month', 'X_smoothed_median_norm_month']   \n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "\n",
    "         # Load in dataset from pickle\n",
    "        with open(f'Pickles/{dataset_name}.pickle', 'rb') as handle:\n",
    "            dataset = pickle.load(handle)\n",
    "        \n",
    "        dataset = np.array(dataset)\n",
    "\n",
    "        print(f'Working on dataset: {dataset_name}')\n",
    "\n",
    "        accuracies = []\n",
    "        sensitivities = []\n",
    "        false_positive_rates = []\n",
    "        specificities = []\n",
    "        precisions = []\n",
    "\n",
    "        fold = 1\n",
    "\n",
    "        for train_index, test_index in outer_cv.split(dataset):\n",
    "\n",
    "            # Print current progress\n",
    "            print(f'Working on fold: {fold}')\n",
    "            fold += 1\n",
    "            \n",
    "            X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "            Y_train, Y_test = abnormal_encoded[train_index], abnormal_encoded[test_index]\n",
    "            \n",
    "            model = create_model()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            \n",
    "            #Fit the model\n",
    "            print('Fitting model')\n",
    "            model.fit(X_train, Y_train, epochs=5, batch_size=4, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "            # Predict the test set\n",
    "            print('Predicting test set')\n",
    "            Y_pred = model.predict(X_test)\n",
    "\n",
    "            Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "            Y_test_classes = np.argmax(Y_test, axis=1)\n",
    "            \n",
    "            # Calulate accuracy, sensitivity, false positive rate, specificity and precision\n",
    "            accuracies.append(accuracy_score(Y_test_classes, Y_pred_classes))\n",
    "            sensitivities.append(recall_score(Y_test_classes, Y_pred_classes,  average='macro'))\n",
    "            false_positive_rates.append(1 - specificity_score(Y_test, Y_pred))\n",
    "            specificities.append(specificity_score(Y_test, Y_pred))\n",
    "            precisions.append(precision_score(Y_test_classes, Y_pred_classes,  average='macro'))\n",
    "\n",
    "\n",
    "\n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        avg_sensitivity = np.mean(sensitivities)\n",
    "        avg_false_positive_rate = np.mean(false_positive_rates)\n",
    "        avg_specificity = np.mean(specificities)\n",
    "        avg_precision = np.mean(precisions)\n",
    "\n",
    "        std_accuracy = np.std(accuracies)\n",
    "        std_sensitivity = np.std(sensitivities)\n",
    "        std_false_positive_rate = np.std(false_positive_rates)\n",
    "        std_specificity = np.std(specificities)\n",
    "        std_precision = np.std(precisions)\n",
    "\n",
    "        dataset_results[dataset_name]['Accuracy'] = (avg_accuracy, std_accuracy)\n",
    "        dataset_results[dataset_name]['Sensitivity'] = (avg_sensitivity, std_sensitivity)\n",
    "        dataset_results[dataset_name]['False Positive Rate'] = (avg_false_positive_rate, std_false_positive_rate)\n",
    "        dataset_results[dataset_name]['Specificity'] = (avg_specificity, std_specificity)\n",
    "        dataset_results[dataset_name]['Precision'] = (avg_precision, std_precision)\n",
    "\n",
    "        # Delete dataset to free up memory\n",
    "        del dataset\n",
    "        del Y_pred\n",
    "\n",
    "train_and_evaluate()\n",
    "print(dataset_results)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: X_smoothed_mean_norm_month\n",
      "Accuracy: 0.696780303030303 +/- 0.09978095269929742\n",
      "Sensitivity: 0.6442111633599048 +/- 0.07147403266931529\n",
      "False Positive Rate: 0.1610282227307399 +/- 0.08355097822644932\n",
      "Specificity: 0.8389717772692601 +/- 0.08355097822644932\n",
      "Precision: 0.6796386528957743 +/- 0.06602168250829586\n",
      "\n",
      "\n",
      "Dataset: X_smoothed_median_norm_month\n",
      "Accuracy: 0.602840909090909 +/- 0.07257174421116831\n",
      "Sensitivity: 0.5478065649072514 +/- 0.028490875610738826\n",
      "False Positive Rate: 0.26574218154080853 +/- 0.1448530873374119\n",
      "Specificity: 0.7342578184591915 +/- 0.1448530873374119\n",
      "Precision: 0.5717232183021658 +/- 0.05136835587495588\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print dataset results in a nice way:\n",
    "for dataset_name, results in dataset_results.items():\n",
    "    print(f'Dataset: {dataset_name}')\n",
    "    for metric, (avg, std) in results.items():\n",
    "        print(f'{metric}: {avg} +/- {std}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dataset: X_smoothed_mean_norm_month\n",
      "Working on fold: 1\n",
      "Fitting model\n",
      "Train on 102 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "102/102 [==============================] - 4s 35ms/sample - loss: 74.6406 - acc: 0.4902 - val_loss: 49.1043 - val_acc: 0.7308\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 3s 28ms/sample - loss: 14.6570 - acc: 0.7745 - val_loss: 18.8501 - val_acc: 0.6154\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 3.5550 - acc: 0.9020 - val_loss: 59.1550 - val_acc: 0.6923\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 3.1558 - acc: 0.8627 - val_loss: 56.9283 - val_acc: 0.7692\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 3s 28ms/sample - loss: 8.7409 - acc: 0.7745 - val_loss: 26.5071 - val_acc: 0.6538\n",
      "Predicting test set\n",
      "Working on fold: 2\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 34ms/sample - loss: 43.7547 - acc: 0.4854 - val_loss: 23.8946 - val_acc: 0.3077\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 7.6204 - acc: 0.6893 - val_loss: 28.2239 - val_acc: 0.6154\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 29ms/sample - loss: 4.3305 - acc: 0.8252 - val_loss: 10.9191 - val_acc: 0.6538\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 1.1941 - acc: 0.9126 - val_loss: 21.6552 - val_acc: 0.6154\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 0.9782 - acc: 0.9029 - val_loss: 30.9435 - val_acc: 0.6538\n",
      "Predicting test set\n",
      "Working on fold: 3\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 33ms/sample - loss: 56.1006 - acc: 0.5243 - val_loss: 89.1823 - val_acc: 0.6154\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 12.3380 - acc: 0.7476 - val_loss: 36.2933 - val_acc: 0.7308\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 6.1374 - acc: 0.8058 - val_loss: 41.4868 - val_acc: 0.5769\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 6.8341 - acc: 0.8350 - val_loss: 65.8069 - val_acc: 0.6538\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 3.8334 - acc: 0.8544 - val_loss: 49.6632 - val_acc: 0.5769\n",
      "Predicting test set\n",
      "Working on fold: 4\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 4s 35ms/sample - loss: 22.9928 - acc: 0.6019 - val_loss: 16.3707 - val_acc: 0.3846\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 12.6334 - acc: 0.6408 - val_loss: 26.0790 - val_acc: 0.1923\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 10.4571 - acc: 0.7476 - val_loss: 24.7656 - val_acc: 0.7308\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.7867 - acc: 0.9126 - val_loss: 16.1490 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.9860e-07 - acc: 1.0000 - val_loss: 17.8733 - val_acc: 0.6923\n",
      "Predicting test set\n",
      "Working on fold: 5\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 33ms/sample - loss: 30.7418 - acc: 0.5146 - val_loss: 15.7886 - val_acc: 0.6154\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 6.9331 - acc: 0.7864 - val_loss: 16.5306 - val_acc: 0.6923\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 3.4070 - acc: 0.8252 - val_loss: 19.6575 - val_acc: 0.6923\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 1.9040 - acc: 0.9320 - val_loss: 24.4841 - val_acc: 0.6923\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 0.2040 - acc: 0.9806 - val_loss: 16.3695 - val_acc: 0.5769\n",
      "Predicting test set\n",
      "Working on dataset: X_smoothed_median_norm_month\n",
      "Working on fold: 1\n",
      "Fitting model\n",
      "Train on 102 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "102/102 [==============================] - 3s 34ms/sample - loss: 23.5784 - acc: 0.5098 - val_loss: 31.9300 - val_acc: 0.7692\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 10.7229 - acc: 0.7941 - val_loss: 54.2704 - val_acc: 0.7692\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 7.2846 - acc: 0.7843 - val_loss: 24.5181 - val_acc: 0.4615\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 3.1941 - acc: 0.9412 - val_loss: 26.3930 - val_acc: 0.5769\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 3s 27ms/sample - loss: 1.2378 - acc: 0.9608 - val_loss: 31.7231 - val_acc: 0.6538\n",
      "Predicting test set\n",
      "Working on fold: 2\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 32ms/sample - loss: 20.7092 - acc: 0.5243 - val_loss: 22.6548 - val_acc: 0.2692\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 10.4846 - acc: 0.7573 - val_loss: 20.0649 - val_acc: 0.5769\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 1.7212 - acc: 0.9320 - val_loss: 16.6410 - val_acc: 0.6154\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 0.5180 - acc: 0.9417 - val_loss: 23.0745 - val_acc: 0.6154\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 0.1479 - acc: 0.9806 - val_loss: 16.7863 - val_acc: 0.5769\n",
      "Predicting test set\n",
      "Working on fold: 3\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 32ms/sample - loss: 51.9144 - acc: 0.6117 - val_loss: 41.1987 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 31.3642 - acc: 0.6893 - val_loss: 66.1716 - val_acc: 0.3462\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 5.5445 - acc: 0.8738 - val_loss: 52.8537 - val_acc: 0.5769\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 2.4200 - acc: 0.9223 - val_loss: 36.3973 - val_acc: 0.6154\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 28ms/sample - loss: 0.5582 - acc: 0.9612 - val_loss: 50.3460 - val_acc: 0.6154\n",
      "Predicting test set\n",
      "Working on fold: 4\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 4s 34ms/sample - loss: 45.0025 - acc: 0.4951 - val_loss: 46.4580 - val_acc: 0.2308\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 11.7805 - acc: 0.7670 - val_loss: 35.7038 - val_acc: 0.5769\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 6.7731 - acc: 0.8447 - val_loss: 42.4703 - val_acc: 0.4231\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.9077 - acc: 0.9320 - val_loss: 55.9636 - val_acc: 0.7692\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.6393 - acc: 0.9126 - val_loss: 28.5867 - val_acc: 0.7308\n",
      "Predicting test set\n",
      "Working on fold: 5\n",
      "Fitting model\n",
      "Train on 103 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "103/103 [==============================] - 3s 33ms/sample - loss: 25.5811 - acc: 0.5340 - val_loss: 10.6116 - val_acc: 0.6154\n",
      "Epoch 2/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 12.9863 - acc: 0.7282 - val_loss: 20.7851 - val_acc: 0.4231\n",
      "Epoch 3/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 5.1176 - acc: 0.8447 - val_loss: 32.3692 - val_acc: 0.6154\n",
      "Epoch 4/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 2.4648 - acc: 0.9320 - val_loss: 44.5576 - val_acc: 0.6923\n",
      "Epoch 5/5\n",
      "103/103 [==============================] - 3s 27ms/sample - loss: 1.9704 - acc: 0.9515 - val_loss: 44.5071 - val_acc: 0.6923\n",
      "Predicting test set\n",
      "Dataset: X_smoothed_mean_norm_month\n",
      "Accuracy: 0.6647727272727273 +/- 0.0838910401149625\n",
      "Sensitivity: 0.6190784653942548 +/- 0.05380697436809445\n",
      "False Positive Rate: 0.2847368421052632 +/- 0.17972277821219615\n",
      "Specificity: 0.7152631578947368 +/- 0.17972277821219615\n",
      "Precision: 0.6708892921960073 +/- 0.1254850274162541\n",
      "\n",
      "\n",
      "Dataset: X_smoothed_median_norm_month\n",
      "Accuracy: 0.5964015151515152 +/- 0.08064135064236685\n",
      "Sensitivity: 0.569601900514944 +/- 0.05950890215341311\n",
      "False Positive Rate: 0.22776811594202898 +/- 0.23781660333121363\n",
      "Specificity: 0.772231884057971 +/- 0.23781660333121363\n",
      "Precision: 0.6489949882853109 +/- 0.11374857902563865\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters to be recorded, along with standard deviation:\n",
    "# Accuracy \n",
    "# Sensitivity(Recall) \n",
    "# False Positive Rate \n",
    "# Specificity \n",
    "# Precision\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "def create_model(filters=32, kernel_size=3, dropout_rate=0.5):    \n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0., input_shape=(19301, 17)))  # Adjust the input_shape to match your dataset\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu', input_shape=(19301, 17)))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))  # Reduced the number of neurons in the dense layer\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_bin = np.argmax(y_pred, axis=1)\n",
    "    y_true_bin = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    tn = np.sum((y_true_bin == 0) & (y_pred_bin == 0))\n",
    "    fp = np.sum((y_true_bin == 0) & (y_pred_bin != 0))\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    return specificity\n",
    "\n",
    "# Create a nested dictionary to store the results\n",
    "dataset_results = {'X_smoothed_mean_norm_month': {}, 'X_smoothed_median_norm_month': {}}\n",
    "\n",
    " # Load in dataset from pickle\n",
    "with open('Pickles/abnormal_encoded.pickle', 'rb') as handle:\n",
    "    abnormal_encoded = pickle.load(handle)\n",
    "\n",
    "def train_and_evaluate():\n",
    "\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Find best dataset to train and test model on\n",
    "    dataset_names = ['X_smoothed_mean_norm_month', 'X_smoothed_median_norm_month']   \n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "\n",
    "         # Load in dataset from pickle\n",
    "        with open(f'Pickles/{dataset_name}.pickle', 'rb') as handle:\n",
    "            dataset = pickle.load(handle)\n",
    "        \n",
    "        dataset = np.array(dataset)\n",
    "\n",
    "        print(f'Working on dataset: {dataset_name}')\n",
    "\n",
    "        accuracies = []\n",
    "        sensitivities = []\n",
    "        false_positive_rates = []\n",
    "        specificities = []\n",
    "        precisions = []\n",
    "\n",
    "        fold = 1\n",
    "\n",
    "        for train_index, test_index in outer_cv.split(dataset):\n",
    "\n",
    "            # Print current progress\n",
    "            print(f'Working on fold: {fold}')\n",
    "            fold += 1\n",
    "            \n",
    "            X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "            Y_train, Y_test = abnormal_encoded[train_index], abnormal_encoded[test_index]\n",
    "\n",
    "            Y_train_classes = np.argmax(Y_train, axis=1)\n",
    "\n",
    "            # Compute class weights\n",
    "            class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                  classes=np.unique(Y_train_classes),\n",
    "                                                  y=Y_train_classes)\n",
    "\n",
    "            class_weights_dict = dict(enumerate(class_weights))\n",
    "            \n",
    "            model = create_model()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "            \n",
    "            #Fit the model\n",
    "            print('Fitting model')\n",
    "            model.fit(X_train, Y_train, epochs=5, batch_size=4, validation_split=0.2, callbacks=[early_stopping], verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "            # Predict the test set\n",
    "            print('Predicting test set')\n",
    "            Y_pred = model.predict(X_test)\n",
    "\n",
    "            Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "            Y_test_classes = np.argmax(Y_test, axis=1)\n",
    "            \n",
    "            # Calulate accuracy, sensitivity, false positive rate, specificity and precision\n",
    "            accuracies.append(accuracy_score(Y_test_classes, Y_pred_classes))\n",
    "            sensitivities.append(recall_score(Y_test_classes, Y_pred_classes,  average='macro'))\n",
    "            false_positive_rates.append(1 - specificity_score(Y_test, Y_pred))\n",
    "            specificities.append(specificity_score(Y_test, Y_pred))\n",
    "            precisions.append(precision_score(Y_test_classes, Y_pred_classes,  average='macro'))\n",
    "\n",
    "\n",
    "\n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        avg_sensitivity = np.mean(sensitivities)\n",
    "        avg_false_positive_rate = np.mean(false_positive_rates)\n",
    "        avg_specificity = np.mean(specificities)\n",
    "        avg_precision = np.mean(precisions)\n",
    "\n",
    "        std_accuracy = np.std(accuracies)\n",
    "        std_sensitivity = np.std(sensitivities)\n",
    "        std_false_positive_rate = np.std(false_positive_rates)\n",
    "        std_specificity = np.std(specificities)\n",
    "        std_precision = np.std(precisions)\n",
    "\n",
    "        dataset_results[dataset_name]['Accuracy'] = (avg_accuracy, std_accuracy)\n",
    "        dataset_results[dataset_name]['Sensitivity'] = (avg_sensitivity, std_sensitivity)\n",
    "        dataset_results[dataset_name]['False Positive Rate'] = (avg_false_positive_rate, std_false_positive_rate)\n",
    "        dataset_results[dataset_name]['Specificity'] = (avg_specificity, std_specificity)\n",
    "        dataset_results[dataset_name]['Precision'] = (avg_precision, std_precision)\n",
    "\n",
    "        # Delete dataset to free up memory\n",
    "        del dataset\n",
    "        del Y_pred\n",
    "\n",
    "train_and_evaluate()\n",
    "\n",
    "# print dataset results in a nice way:\n",
    "for dataset_name, results in dataset_results.items():\n",
    "    print(f'Dataset: {dataset_name}')\n",
    "    for metric, (avg, std) in results.items():\n",
    "        print(f'{metric}: {avg} +/- {std}')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: tensorflow-directml 1.15.8 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.21.6 which is incompatible.\n",
      "ERROR: tensorflow-directml 1.15.8 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.11.2 which is incompatible.\n",
      "ERROR: tensorflow-directml 1.15.8 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.11.0 which is incompatible.\n",
      "  WARNING: The script tensorboard.exe is installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached tensorflow-2.11.0-cp37-cp37m-win_amd64.whl (1.9 kB)\n",
      "Requirement already up-to-date: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.21.6)\n",
      "Collecting tensorflow-intel==2.11.0; platform_system == \"Windows\"\n",
      "  Using cached tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl (266.3 MB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: astunparse>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<3.20,>=3.9.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (3.19.6)\n",
      "Requirement already satisfied, skipping upgrade: h5py>=2.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (0.2.2)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied, skipping upgrade: packaging in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (23.2)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (4.7.1)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2.0,>=1.24.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (1.60.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (47.1.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Requirement already satisfied, skipping upgrade: keras<2.12,>=2.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (2.11.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel<1.0,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (0.42.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (0.4.6)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (3.4.4)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (2.31.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<3,>=1.6.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (2.27.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (1.8.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (2.2.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=4.4; python_version < \"3.10\" in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (6.7.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (3.6)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (3.3.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (2.0.7)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<6.0,>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (5.3.2)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (4.9)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (2.1.4)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (3.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.6.0,>=0.4.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0; platform_system == \"Windows\"->tensorflow) (0.5.1)\n",
      "Installing collected packages: tensorflow-estimator, tensorflow-io-gcs-filesystem, tensorboard, libclang, flatbuffers, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "Successfully installed flatbuffers-23.5.26 libclang-16.0.6 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.31.0\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tensorflow numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dataset: X_smoothed_mean_norm\n",
      "Working on fold: 1\n",
      "Fitting model\n",
      "Epoch 1/5\n",
      "22/22 [==============================] - 67s 3s/step - loss: 1.0764 - accuracy: 0.3882 - val_loss: 0.8068 - val_accuracy: 0.7273\n",
      "Epoch 2/5\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.7792 - accuracy: 0.6588 - val_loss: 0.6784 - val_accuracy: 0.7273\n",
      "Epoch 3/5\n",
      "22/22 [==============================] - 73s 3s/step - loss: 0.6888 - accuracy: 0.6471 - val_loss: 0.6460 - val_accuracy: 0.7273\n",
      "Epoch 4/5\n",
      "22/22 [==============================] - 81s 4s/step - loss: 0.6494 - accuracy: 0.6706 - val_loss: 0.6272 - val_accuracy: 0.7273\n",
      "Epoch 5/5\n",
      "22/22 [==============================] - 86s 4s/step - loss: 0.6216 - accuracy: 0.6824 - val_loss: 0.6221 - val_accuracy: 0.7273\n",
      "Predicting test set\n",
      "2/2 [==============================] - 3s 1s/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "specificity_score() got an unexpected keyword argument 'average'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5656\\4282330451.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;31m# print dataset results in a nice way:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5656\\4282330451.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0msensitivities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mfalse_positive_rates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mspecificity_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0mspecificities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspecificity_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mprecisions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: specificity_score() got an unexpected keyword argument 'average'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parameters to be recorded, along with standard deviation:\n",
    "# Accuracy \n",
    "# Sensitivity(Recall) \n",
    "# False Positive Rate \n",
    "# Specificity \n",
    "# Precision\n",
    "\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Alternatively, to suppress all warnings including those not from TensorFlow\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# If you also want to suppress warnings from other libraries, you can use\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "def create_model(filters=32, kernel_size=3, dropout_rate=0.5):    \n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0., input_shape=(19301, 16)))  # Adjust the input_shape to match your dataset\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu', input_shape=(19301, 16)))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))  # Reduced the number of neurons in the dense layer\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "\n",
    "def create_lstm_model(input_shape=(19301, 16), lstm_units=32, dropout_rate=0.5, output_classes=3):\n",
    "    model = Sequential()\n",
    "    # Masking layer to ignore the padded values\n",
    "    model.add(Masking(mask_value=0., input_shape=input_shape))\n",
    "    \n",
    "    # LSTM layer\n",
    "    model.add(LSTM(lstm_units, return_sequences=False))  # 'return_sequences=False' because we only need the last output\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # A Dense layer for further processing\n",
    "    model.add(Dense(lstm_units, activation='relu'))\n",
    "    \n",
    "    # The output layer with softmax activation for classification\n",
    "    model.add(Dense(output_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def create_simple_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(19301, 16)))  # Adjust input_shape as needed\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_bin = np.argmax(y_pred, axis=1)\n",
    "    y_true_bin = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    tn = np.sum((y_true_bin == 0) & (y_pred_bin == 0))\n",
    "    fp = np.sum((y_true_bin == 0) & (y_pred_bin != 0))\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    return specificity\n",
    "\n",
    "# Create a nested dictionary to store the results\n",
    "dataset_results = {'X_smoothed_mean_norm': {}, 'X_smoothed_median_norm': {}}\n",
    "\n",
    " # Load in dataset from pickle\n",
    "with open('Pickles/abnormal_encoded.pickle', 'rb') as handle:\n",
    "    abnormal_encoded = pickle.load(handle)\n",
    "\n",
    "@tf.autograph.experimental.do_not_convert\n",
    "def train_and_evaluate():\n",
    "\n",
    "    outer_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Find best dataset to train and test model on\n",
    "    dataset_names = ['X_smoothed_mean_norm']   \n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "\n",
    "         # Load in dataset from pickle\n",
    "        with open(f'Pickles/{dataset_name}.pickle', 'rb') as handle:\n",
    "            dataset = pickle.load(handle)\n",
    "        \n",
    "        dataset = np.array(dataset)\n",
    "\n",
    "        print(f'Working on dataset: {dataset_name}')\n",
    "\n",
    "        accuracies = []\n",
    "        sensitivities = []\n",
    "        false_positive_rates = []\n",
    "        specificities = []\n",
    "        precisions = []\n",
    "\n",
    "        fold = 1\n",
    "\n",
    "        for train_index, test_index in outer_cv.split(dataset):\n",
    "\n",
    "            # Print current progress\n",
    "            print(f'Working on fold: {fold}')\n",
    "            fold += 1\n",
    "            \n",
    "            X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "            Y_train, Y_test = abnormal_encoded[train_index], abnormal_encoded[test_index]\n",
    "\n",
    "            X_train = X_train.astype('float32')\n",
    "            Y_train = Y_train.astype('float32')\n",
    "            X_test = X_test.astype('float32')\n",
    "            Y_test = Y_test.astype('float32')\n",
    "\n",
    "            \n",
    "            model = create_simple_lstm_model()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            \n",
    "            #Fit the model\n",
    "            print('Fitting model')\n",
    "            model.fit(X_train, Y_train, epochs=5, batch_size=4, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "            # Predict the test set\n",
    "            print('Predicting test set')\n",
    "            Y_pred = model.predict(X_test)\n",
    "\n",
    "            Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "            Y_test_classes = np.argmax(Y_test, axis=1)\n",
    "            \n",
    "            # Calulate accuracy, sensitivity, false positive rate, specificity and precision\n",
    "            accuracies.append(accuracy_score(Y_test_classes, Y_pred_classes))\n",
    "            sensitivities.append(recall_score(Y_test_classes, Y_pred_classes, average='macro'))\n",
    "            false_positive_rates.append(1 - specificity_score(Y_test, Y_pred, average='macro'))\n",
    "            specificities.append(specificity_score(Y_test, Y_pred))\n",
    "            precisions.append(precision_score(Y_test_classes, Y_pred_classes, average='macro'))\n",
    "\n",
    "\n",
    "\n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        avg_sensitivity = np.mean(sensitivities)\n",
    "        avg_false_positive_rate = np.mean(false_positive_rates)\n",
    "        avg_specificity = np.mean(specificities)\n",
    "        avg_precision = np.mean(precisions)\n",
    "\n",
    "        std_accuracy = np.std(accuracies)\n",
    "        std_sensitivity = np.std(sensitivities)\n",
    "        std_false_positive_rate = np.std(false_positive_rates)\n",
    "        std_specificity = np.std(specificities)\n",
    "        std_precision = np.std(precisions)\n",
    "\n",
    "        dataset_results[dataset_name]['Accuracy'] = (avg_accuracy, std_accuracy)\n",
    "        dataset_results[dataset_name]['Sensitivity'] = (avg_sensitivity, std_sensitivity)\n",
    "        dataset_results[dataset_name]['False Positive Rate'] = (avg_false_positive_rate, std_false_positive_rate)\n",
    "        dataset_results[dataset_name]['Specificity'] = (avg_specificity, std_specificity)\n",
    "        dataset_results[dataset_name]['Precision'] = (avg_precision, std_precision)\n",
    "\n",
    "        # Delete dataset to free up memory\n",
    "        del dataset\n",
    "        del Y_pred\n",
    "\n",
    "train_and_evaluate()\n",
    "# print dataset results in a nice way:\n",
    "for dataset_name, results in dataset_results.items():\n",
    "    print(f'Dataset: {dataset_name}')\n",
    "    for metric, (avg, std) in results.items():\n",
    "        print(f'{metric}: {avg} +/- {std}')\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(dataset_names, create_model_fn):\n",
    "\n",
    "    outer_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Find best dataset to train and test model on\n",
    "    #dataset_names = ['X_smoothed_mean_norm']   \n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "\n",
    "         # Load in dataset from pickle\n",
    "        with open(f'Pickles/{dataset_name}.pickle', 'rb') as handle:\n",
    "            dataset = pickle.load(handle)\n",
    "        \n",
    "        dataset = np.array(dataset)\n",
    "\n",
    "        print(f'Working on dataset: {dataset_name}')\n",
    "\n",
    "        accuracies = []\n",
    "        sensitivities = []\n",
    "        false_positive_rates = []\n",
    "        specificities = []\n",
    "        precisions = []\n",
    "\n",
    "        fold = 1\n",
    "\n",
    "        for train_index, test_index in outer_cv.split(dataset):\n",
    "\n",
    "            # Print current progress\n",
    "            print(f'Working on fold: {fold}')\n",
    "            fold += 1\n",
    "            \n",
    "            X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "            Y_train, Y_test = abnormal_encoded[train_index], abnormal_encoded[test_index]\n",
    "\n",
    "            X_train = X_train.astype('float32')\n",
    "            Y_train = Y_train.astype('float32')\n",
    "            X_test = X_test.astype('float32')\n",
    "            Y_test = Y_test.astype('float32')\n",
    "\n",
    "            \n",
    "            model = create_model_fn()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            \n",
    "            #Fit the model\n",
    "            print('Fitting model')\n",
    "            model.fit(X_train, Y_train, epochs=5, batch_size=4, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "            # Predict the test set\n",
    "            print('Predicting test set')\n",
    "            Y_pred = model.predict(X_test)\n",
    "\n",
    "            Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "            Y_test_classes = np.argmax(Y_test, axis=1)\n",
    "            \n",
    "            # Calulate accuracy, sensitivity, false positive rate, specificity and precision\n",
    "            accuracies.append(accuracy_score(Y_test_classes, Y_pred_classes))\n",
    "            sensitivities.append(recall_score(Y_test_classes, Y_pred_classes, average='macro'))\n",
    "            false_positive_rates.append(1 - specificity_score(Y_test, Y_pred, average='macro'))\n",
    "            specificities.append(specificity_score(Y_test, Y_pred))\n",
    "            precisions.append(precision_score(Y_test_classes, Y_pred_classes, average='macro'))\n",
    "\n",
    "\n",
    "\n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        avg_sensitivity = np.mean(sensitivities)\n",
    "        avg_false_positive_rate = np.mean(false_positive_rates)\n",
    "        avg_specificity = np.mean(specificities)\n",
    "        avg_precision = np.mean(precisions)\n",
    "\n",
    "        std_accuracy = np.std(accuracies)\n",
    "        std_sensitivity = np.std(sensitivities)\n",
    "        std_false_positive_rate = np.std(false_positive_rates)\n",
    "        std_specificity = np.std(specificities)\n",
    "        std_precision = np.std(precisions)\n",
    "\n",
    "        dataset_results[dataset_name]['Accuracy'] = (avg_accuracy, std_accuracy)\n",
    "        dataset_results[dataset_name]['Sensitivity'] = (avg_sensitivity, std_sensitivity)\n",
    "        dataset_results[dataset_name]['False Positive Rate'] = (avg_false_positive_rate, std_false_positive_rate)\n",
    "        dataset_results[dataset_name]['Specificity'] = (avg_specificity, std_specificity)\n",
    "        dataset_results[dataset_name]['Precision'] = (avg_precision, std_precision)\n",
    "\n",
    "        # Delete dataset to free up memory\n",
    "        del dataset\n",
    "        del Y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dataset: X_smoothed_mean_norm\n",
      "Working on fold: 1\n",
      "Fitting model\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5656\\4084186744.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_smoothed_mean_norm'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcreate_lstm_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5656\\3063681656.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(dataset_names, create_model_fn)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m#Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Fitting model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m# Predict the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    943\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[1;31m# no_variable_creation function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m       _, _, filtered_flat_args = (\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m--> 135\u001b[1;33m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 53\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_evaluate(['X_smoothed_mean_norm'] , create_lstm_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
