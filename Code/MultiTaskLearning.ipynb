{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''RUN THIS CELL FIRST IF ON LOCAL MACHINE'''\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.enable_eager_execution(tf.ConfigProto(log_device_placement=False))\n",
        "\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T4ilQacfTSVa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Masking\n",
        "from sklearn.model_selection import KFold\n",
        "# import early stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.utils import class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCwbTCmKTYQQ",
        "outputId": "2a121591-5f1c-49fe-eb5f-a6c2b0144dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g4GzeYlTUia-"
      },
      "outputs": [],
      "source": [
        "def specificity_score(y_true, y_pred):\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    y_pred_bin = np.argmax(y_pred, axis=1)\n",
        "    y_true_bin = np.argmax(y_true, axis=1)\n",
        "\n",
        "    tn = np.sum((y_true_bin == 0) & (y_pred_bin == 0))\n",
        "    fp = np.sum((y_true_bin == 0) & (y_pred_bin != 0))\n",
        "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
        "    return specificity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BVVJb1gkTSVb"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(dataset_names, create_model_fn, class_weights):\n",
        "\n",
        "    outer_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "    # Find best dataset to train and test model on\n",
        "    #dataset_names = ['X_smoothed_mean_norm']\n",
        "\n",
        "    for dataset_name in dataset_names:\n",
        "\n",
        "         # Load in dataset from pickle\n",
        "        with open(f'drive/MyDrive/Pickles/{dataset_name}.pickle', 'rb') as handle:\n",
        "            dataset = pickle.load(handle)\n",
        "\n",
        "        dataset = np.array(dataset)\n",
        "\n",
        "        print(f'Working on dataset: {dataset_name}')\n",
        "\n",
        "        accuracies = []\n",
        "        sensitivities = []\n",
        "        false_positive_rates = []\n",
        "        specificities = []\n",
        "        precisions = []\n",
        "\n",
        "        fold = 1\n",
        "\n",
        "        for train_index, test_index in outer_cv.split(dataset):\n",
        "\n",
        "            # Print current progress\n",
        "            print(f'Working on fold: {fold}')\n",
        "            fold += 1\n",
        "\n",
        "            X_train, X_test = dataset[train_index], dataset[test_index]\n",
        "            Y_train, Y_test = abnormal_encoded[train_index], abnormal_encoded[test_index]\n",
        "\n",
        "            X_train = X_train.astype('float32')\n",
        "            Y_train = Y_train.astype('float32')\n",
        "            X_test = X_test.astype('float32')\n",
        "            Y_test = Y_test.astype('float32')\n",
        "\n",
        "            model = create_model_fn(X_train.shape[1:])\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "            if class_weights:\n",
        "\n",
        "                Y_train_classes = np.argmax(Y_train, axis=1)\n",
        "\n",
        "                # Compute class weights\n",
        "                cw = class_weight.compute_class_weight('balanced',\n",
        "                                                    classes=np.unique(Y_train_classes),\n",
        "                                                    y=Y_train_classes)\n",
        "\n",
        "                class_weights_dict = dict(enumerate(cw))\n",
        "\n",
        "                #Fit the model\n",
        "                print('Fitting model')\n",
        "                model.fit(X_train, Y_train, epochs=5, batch_size=4, validation_split=0.2, callbacks=[early_stopping], verbose=1, class_weight=class_weights_dict)\n",
        "\n",
        "            else:\n",
        "                #Fit the model\n",
        "                print('Fitting model')\n",
        "                model.fit(X_train, Y_train, epochs=5, batch_size=4, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "            # Predict the test set\n",
        "            print('Predicting test set')\n",
        "            Y_pred = model.predict(X_test)\n",
        "\n",
        "            Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "            Y_test_classes = np.argmax(Y_test, axis=1)\n",
        "\n",
        "            # Calulate accuracy, sensitivity, false positive rate, specificity and precision\n",
        "            accuracies.append(accuracy_score(Y_test_classes, Y_pred_classes))\n",
        "            sensitivities.append(recall_score(Y_test_classes, Y_pred_classes, average='macro'))\n",
        "            false_positive_rates.append(1 - specificity_score(Y_test, Y_pred))\n",
        "            specificities.append(specificity_score(Y_test, Y_pred))\n",
        "            precisions.append(precision_score(Y_test_classes, Y_pred_classes, average='macro'))\n",
        "\n",
        "\n",
        "\n",
        "        avg_accuracy = np.mean(accuracies)\n",
        "        avg_sensitivity = np.mean(sensitivities)\n",
        "        avg_false_positive_rate = np.mean(false_positive_rates)\n",
        "        avg_specificity = np.mean(specificities)\n",
        "        avg_precision = np.mean(precisions)\n",
        "\n",
        "        std_accuracy = np.std(accuracies)\n",
        "        std_sensitivity = np.std(sensitivities)\n",
        "        std_false_positive_rate = np.std(false_positive_rates)\n",
        "        std_specificity = np.std(specificities)\n",
        "        std_precision = np.std(precisions)\n",
        "\n",
        "        dataset_results[dataset_name]['Accuracy'] = (avg_accuracy, std_accuracy)\n",
        "        dataset_results[dataset_name]['Sensitivity'] = (avg_sensitivity, std_sensitivity)\n",
        "        dataset_results[dataset_name]['False Positive Rate'] = (avg_false_positive_rate, std_false_positive_rate)\n",
        "        dataset_results[dataset_name]['Specificity'] = (avg_specificity, std_specificity)\n",
        "        dataset_results[dataset_name]['Precision'] = (avg_precision, std_precision)\n",
        "\n",
        "        for dataset_name, results in dataset_results.items():\n",
        "            print(f'Dataset: {dataset_name}')\n",
        "            for metric, (avg, std) in results.items():\n",
        "                print(f'{metric}: {avg} +/- {std}')\n",
        "            print('\\n')\n",
        "\n",
        "\n",
        "        # Delete dataset to free up memory\n",
        "        del dataset\n",
        "        del Y_pred\n",
        "    return dataset_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gL1-AspcTSVb"
      },
      "outputs": [],
      "source": [
        "# Model functions\n",
        "\n",
        "def create_cnn_model(shape, filters=32, kernel_size=3, dropout_rate=0.5):\n",
        "    model = Sequential()\n",
        "    model.add(Masking(mask_value=0., input_shape=shape))  # Adjust the input_shape to match your dataset\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu', input_shape=(19301, 16)))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(32, activation='relu'))  # Reduced the number of neurons in the dense layer\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_lstm_model(shape, lstm_units=32, dropout_rate=0.5, output_classes=3):\n",
        "    model = Sequential()\n",
        "    model.add(Masking(mask_value=0., input_shape=shape))\n",
        "    model.add(LSTM(lstm_units, return_sequences=False))  # 'return_sequences=False' because we only need the last output\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(lstm_units, activation='relu'))\n",
        "    model.add(Dense(output_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_gru_model(shape, gru_units=32, dropout_rate=0.5, output_classes=3):\n",
        "    model = Sequential()\n",
        "    model.add(Masking(mask_value=0., input_shape=shape))\n",
        "    model.add(GRU(gru_units, return_sequences=False))  # return_sequences=False because we only need the last output\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(gru_units, activation='relu'))\n",
        "    model.add(Dense(output_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-YsRAY00UNnG"
      },
      "outputs": [],
      "source": [
        "'''RUN THIS CELL IF ON COLAB'''\n",
        "\n",
        "# Load in dataset from pickle\n",
        "with open('drive/MyDrive/Pickles/abnormal_encoded.pickle', 'rb') as handle:\n",
        "    abnormal_encoded = pickle.load(handle)\n",
        "\n",
        "abnormal_encoded = np.array(abnormal_encoded)\n",
        "\n",
        "with open('drive/MyDrive/Pickles/AIMS.pickle', 'rb') as handle:\n",
        "    AIMS = pickle.load(handle)\n",
        "\n",
        "AIM = np.array(AIMS)\n",
        "\n",
        "with open('drive/MyDrive/Pickles/Optimality.pickle', 'rb') as handle:\n",
        "    optimality = pickle.load(handle)\n",
        "\n",
        "optimality = np.array(optimality)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVb3aj0TTSVb",
        "outputId": "7633eea4-d805-4412-e30d-011155573867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on dataset: X_smoothed_mean_norm\n",
            "Working on fold: 1\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 2s 30ms/step - loss: 22.3467 - accuracy: 0.4235 - val_loss: 10.9162 - val_accuracy: 0.5909\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 7.6254 - accuracy: 0.7412 - val_loss: 26.1706 - val_accuracy: 0.5000\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 3.9040 - accuracy: 0.9059 - val_loss: 27.9859 - val_accuracy: 0.6818\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.5618 - accuracy: 0.9059 - val_loss: 28.5547 - val_accuracy: 0.5909\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Working on fold: 2\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 21ms/step - loss: 46.1282 - accuracy: 0.5412 - val_loss: 6.0454 - val_accuracy: 0.7273\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 6.9497 - accuracy: 0.7176 - val_loss: 10.0646 - val_accuracy: 0.5000\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 5.4074 - accuracy: 0.7412 - val_loss: 6.0894 - val_accuracy: 0.5455\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.5195 - accuracy: 0.9529 - val_loss: 6.3230 - val_accuracy: 0.5455\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Working on fold: 3\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 2s 26ms/step - loss: 46.0135 - accuracy: 0.4651 - val_loss: 37.8708 - val_accuracy: 0.6818\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 6.9466 - accuracy: 0.8023 - val_loss: 21.8819 - val_accuracy: 0.5909\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0126 - accuracy: 0.9419 - val_loss: 38.5756 - val_accuracy: 0.6364\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5546 - accuracy: 0.9767 - val_loss: 26.5844 - val_accuracy: 0.5909\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9489 - accuracy: 0.9419 - val_loss: 32.0481 - val_accuracy: 0.6818\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Dataset: X_smoothed_mean_norm_month\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm_month\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_mean_norm\n",
            "Accuracy: 0.6024924295364547 +/- 0.022698545586030232\n",
            "Sensitivity: 0.5954031005114596 +/- 0.026547838177203346\n",
            "False Positive Rate: 0.35944121238238885 +/- 0.05336288150401101\n",
            "Specificity: 0.6405587876176112 +/- 0.05336288150401101\n",
            "Precision: 0.5906391501219087 +/- 0.03294091467480022\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm\n",
            "\n",
            "\n",
            "Working on dataset: X_smoothed_mean_norm_month\n",
            "Working on fold: 1\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 3s 54ms/step - loss: 76.3911 - accuracy: 0.5176 - val_loss: 79.9712 - val_accuracy: 0.2727\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 54.5821 - accuracy: 0.4824 - val_loss: 37.4107 - val_accuracy: 0.7273\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 7.3997 - accuracy: 0.7294 - val_loss: 38.2822 - val_accuracy: 0.6364\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.5012 - accuracy: 0.9294 - val_loss: 36.6740 - val_accuracy: 0.6364\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.1571 - accuracy: 0.9765 - val_loss: 25.2022 - val_accuracy: 0.6364\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "Working on fold: 2\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 3s 35ms/step - loss: 55.8808 - accuracy: 0.5529 - val_loss: 30.1985 - val_accuracy: 0.8182\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 15.5205 - accuracy: 0.6235 - val_loss: 8.4225 - val_accuracy: 0.5909\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.5697 - accuracy: 0.9176 - val_loss: 7.8450 - val_accuracy: 0.6364\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.8415 - accuracy: 0.9059 - val_loss: 10.1286 - val_accuracy: 0.5000\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 0.8877 - accuracy: 0.9059 - val_loss: 15.2040 - val_accuracy: 0.3636\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Working on fold: 3\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 5s 96ms/step - loss: 66.0465 - accuracy: 0.4651 - val_loss: 42.9594 - val_accuracy: 0.6364\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 26.9324 - accuracy: 0.7326 - val_loss: 60.9355 - val_accuracy: 0.4545\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 14.6442 - accuracy: 0.8023 - val_loss: 44.1150 - val_accuracy: 0.5909\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 4.0480 - accuracy: 0.9070 - val_loss: 46.5847 - val_accuracy: 0.5000\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Dataset: X_smoothed_mean_norm_month\n",
            "Accuracy: 0.5284183554623806 +/- 0.12529679807062674\n",
            "Sensitivity: 0.6054122863720387 +/- 0.05393824439757144\n",
            "False Positive Rate: 0.5861164978812038 +/- 0.2138975519880295\n",
            "Specificity: 0.41388350211879626 +/- 0.2138975519880295\n",
            "Precision: 0.6159892094489617 +/- 0.03942193675566511\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm_month\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_mean_norm\n",
            "Accuracy: 0.6024924295364547 +/- 0.022698545586030232\n",
            "Sensitivity: 0.5954031005114596 +/- 0.026547838177203346\n",
            "False Positive Rate: 0.35944121238238885 +/- 0.05336288150401101\n",
            "Specificity: 0.6405587876176112 +/- 0.05336288150401101\n",
            "Precision: 0.5906391501219087 +/- 0.03294091467480022\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm\n",
            "\n",
            "\n",
            "Working on dataset: X_smoothed_median_norm\n",
            "Working on fold: 1\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 2s 27ms/step - loss: 26.0726 - accuracy: 0.4471 - val_loss: 8.8134 - val_accuracy: 0.5455\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 4.9932 - accuracy: 0.7882 - val_loss: 18.1387 - val_accuracy: 0.5455\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 4.5853 - accuracy: 0.8706 - val_loss: 20.8299 - val_accuracy: 0.5000\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 2.6966 - accuracy: 0.9294 - val_loss: 21.7234 - val_accuracy: 0.6818\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "Working on fold: 2\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 2s 23ms/step - loss: 19.2372 - accuracy: 0.5176 - val_loss: 36.9696 - val_accuracy: 0.3182\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 5.4883 - accuracy: 0.8588 - val_loss: 8.4544 - val_accuracy: 0.6364\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 2.8579 - accuracy: 0.8353 - val_loss: 5.5356 - val_accuracy: 0.5909\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.7122 - accuracy: 0.8824 - val_loss: 6.2783 - val_accuracy: 0.7273\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.6620 - accuracy: 0.9412 - val_loss: 12.1920 - val_accuracy: 0.7727\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Working on fold: 3\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 2s 32ms/step - loss: 6.3287 - accuracy: 0.5465 - val_loss: 7.3701 - val_accuracy: 0.2727\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.6477 - accuracy: 0.6977 - val_loss: 3.3907 - val_accuracy: 0.6364\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3408 - accuracy: 0.8953 - val_loss: 2.4375 - val_accuracy: 0.6364\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.0794 - accuracy: 0.9767 - val_loss: 2.3532 - val_accuracy: 0.6364\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.4417 - val_accuracy: 0.6364\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Dataset: X_smoothed_mean_norm_month\n",
            "Accuracy: 0.5284183554623806 +/- 0.12529679807062674\n",
            "Sensitivity: 0.6054122863720387 +/- 0.05393824439757144\n",
            "False Positive Rate: 0.5861164978812038 +/- 0.2138975519880295\n",
            "Specificity: 0.41388350211879626 +/- 0.2138975519880295\n",
            "Precision: 0.6159892094489617 +/- 0.03942193675566511\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm_month\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_mean_norm\n",
            "Accuracy: 0.6024924295364547 +/- 0.022698545586030232\n",
            "Sensitivity: 0.5954031005114596 +/- 0.026547838177203346\n",
            "False Positive Rate: 0.35944121238238885 +/- 0.05336288150401101\n",
            "Specificity: 0.6405587876176112 +/- 0.05336288150401101\n",
            "Precision: 0.5906391501219087 +/- 0.03294091467480022\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm\n",
            "Accuracy: 0.6583973911017936 +/- 0.06805578421456863\n",
            "Sensitivity: 0.5871742894188714 +/- 0.020542103304235802\n",
            "False Positive Rate: 0.1061732385261797 +/- 0.07248435114464193\n",
            "Specificity: 0.8938267614738202 +/- 0.07248435114464193\n",
            "Precision: 0.6553776792538591 +/- 0.023814390249551697\n",
            "\n",
            "\n",
            "Working on dataset: X_smoothed_median_norm_month\n",
            "Working on fold: 1\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 31.8793 - accuracy: 0.5529 - val_loss: 1.6784 - val_accuracy: 0.7273\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.7180 - accuracy: 0.6824 - val_loss: 0.6530 - val_accuracy: 0.5909\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.3003 - accuracy: 0.9059 - val_loss: 0.9127 - val_accuracy: 0.6364\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.1814 - accuracy: 0.9647 - val_loss: 0.7232 - val_accuracy: 0.6364\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.0926 - accuracy: 1.0000 - val_loss: 0.7813 - val_accuracy: 0.6364\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Working on fold: 2\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 30.2546 - accuracy: 0.5294 - val_loss: 122.6305 - val_accuracy: 0.1818\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 27.8967 - accuracy: 0.7529 - val_loss: 55.0733 - val_accuracy: 0.7727\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 21.1827 - accuracy: 0.8118 - val_loss: 33.1586 - val_accuracy: 0.6818\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 2.1159 - accuracy: 0.9529 - val_loss: 55.8359 - val_accuracy: 0.7727\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 8.9184 - accuracy: 0.8706 - val_loss: 43.5183 - val_accuracy: 0.7273\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Working on fold: 3\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 2s 22ms/step - loss: 37.7312 - accuracy: 0.5930 - val_loss: 32.8900 - val_accuracy: 0.6818\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 22.5131 - accuracy: 0.6744 - val_loss: 45.5256 - val_accuracy: 0.3636\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 10.0733 - accuracy: 0.8140 - val_loss: 26.8875 - val_accuracy: 0.5909\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 4.1225 - accuracy: 0.8953 - val_loss: 28.1037 - val_accuracy: 0.5909\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 2.5288 - accuracy: 0.8953 - val_loss: 27.9516 - val_accuracy: 0.6364\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Dataset: X_smoothed_mean_norm_month\n",
            "Accuracy: 0.5284183554623806 +/- 0.12529679807062674\n",
            "Sensitivity: 0.6054122863720387 +/- 0.05393824439757144\n",
            "False Positive Rate: 0.5861164978812038 +/- 0.2138975519880295\n",
            "Specificity: 0.41388350211879626 +/- 0.2138975519880295\n",
            "Precision: 0.6159892094489617 +/- 0.03942193675566511\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm_month\n",
            "Accuracy: 0.6087817377125554 +/- 0.024707284922990343\n",
            "Sensitivity: 0.5496908750004725 +/- 0.022459985618095736\n",
            "False Positive Rate: 0.21487825899590607 +/- 0.025739180275967184\n",
            "Specificity: 0.7851217410040939 +/- 0.025739180275967184\n",
            "Precision: 0.5611061738147746 +/- 0.029737200420561147\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_mean_norm\n",
            "Accuracy: 0.6024924295364547 +/- 0.022698545586030232\n",
            "Sensitivity: 0.5954031005114596 +/- 0.026547838177203346\n",
            "False Positive Rate: 0.35944121238238885 +/- 0.05336288150401101\n",
            "Specificity: 0.6405587876176112 +/- 0.05336288150401101\n",
            "Precision: 0.5906391501219087 +/- 0.03294091467480022\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm\n",
            "Accuracy: 0.6583973911017936 +/- 0.06805578421456863\n",
            "Sensitivity: 0.5871742894188714 +/- 0.020542103304235802\n",
            "False Positive Rate: 0.1061732385261797 +/- 0.07248435114464193\n",
            "Specificity: 0.8938267614738202 +/- 0.07248435114464193\n",
            "Precision: 0.6553776792538591 +/- 0.023814390249551697\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset_results = {'X_smoothed_mean_norm_month': {}, 'X_smoothed_median_norm_month': {}, 'X_smoothed_mean_norm': {}, 'X_smoothed_median_norm': {}}\n",
        "\n",
        "cnn_dataset_result_cw = train_and_evaluate(['X_smoothed_mean_norm', 'X_smoothed_mean_norm_month','X_smoothed_median_norm', 'X_smoothed_median_norm_month'], create_cnn_model, True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAqA0aGXUBTp",
        "outputId": "6c9bd3e8-d462-44b8-92e9-d0462a4fb256"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on dataset: X_smoothed_mean_norm\n",
            "Working on fold: 1\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 22ms/step - loss: 15.8563 - accuracy: 0.5529 - val_loss: 5.4335 - val_accuracy: 0.6818\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 4.6145 - accuracy: 0.8000 - val_loss: 8.6626 - val_accuracy: 0.6364\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.8578 - accuracy: 0.8824 - val_loss: 19.0609 - val_accuracy: 0.4091\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.1123 - accuracy: 0.9412 - val_loss: 19.5823 - val_accuracy: 0.6818\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Working on fold: 2\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 2s 25ms/step - loss: 30.6900 - accuracy: 0.4588 - val_loss: 8.9566 - val_accuracy: 0.7727\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 2.8553 - accuracy: 0.8824 - val_loss: 5.5423 - val_accuracy: 0.7727\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.3019 - accuracy: 0.9529 - val_loss: 4.0455 - val_accuracy: 0.6818\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.1710 - accuracy: 0.9529 - val_loss: 5.3524 - val_accuracy: 0.7273\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.2669 - accuracy: 0.9529 - val_loss: 8.9192 - val_accuracy: 0.5000\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Working on fold: 3\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 22ms/step - loss: 17.3586 - accuracy: 0.5698 - val_loss: 44.6831 - val_accuracy: 0.3636\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 10.9374 - accuracy: 0.7674 - val_loss: 14.6169 - val_accuracy: 0.5909\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.8936 - accuracy: 0.8837 - val_loss: 13.4980 - val_accuracy: 0.5000\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.1422 - accuracy: 0.9767 - val_loss: 18.8622 - val_accuracy: 0.6364\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.2587 - accuracy: 0.9651 - val_loss: 22.1837 - val_accuracy: 0.6818\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Dataset: X_smoothed_mean_norm_month\n",
            "Accuracy: 0.5284183554623806 +/- 0.12529679807062674\n",
            "Sensitivity: 0.6054122863720387 +/- 0.05393824439757144\n",
            "False Positive Rate: 0.5861164978812038 +/- 0.2138975519880295\n",
            "Specificity: 0.41388350211879626 +/- 0.2138975519880295\n",
            "Precision: 0.6159892094489617 +/- 0.03942193675566511\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm_month\n",
            "Accuracy: 0.6087817377125554 +/- 0.024707284922990343\n",
            "Sensitivity: 0.5496908750004725 +/- 0.022459985618095736\n",
            "False Positive Rate: 0.21487825899590607 +/- 0.025739180275967184\n",
            "Specificity: 0.7851217410040939 +/- 0.025739180275967184\n",
            "Precision: 0.5611061738147746 +/- 0.029737200420561147\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_mean_norm\n",
            "Accuracy: 0.5159562077801071 +/- 0.1165314296316961\n",
            "Sensitivity: 0.5244137871072856 +/- 0.05672229597543082\n",
            "False Positive Rate: 0.3939524527759821 +/- 0.30286980063280217\n",
            "Specificity: 0.6060475472240178 +/- 0.30286980063280217\n",
            "Precision: 0.5217549598623767 +/- 0.07099989175088521\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm\n",
            "Accuracy: 0.6583973911017936 +/- 0.06805578421456863\n",
            "Sensitivity: 0.5871742894188714 +/- 0.020542103304235802\n",
            "False Positive Rate: 0.1061732385261797 +/- 0.07248435114464193\n",
            "Specificity: 0.8938267614738202 +/- 0.07248435114464193\n",
            "Precision: 0.6553776792538591 +/- 0.023814390249551697\n",
            "\n",
            "\n",
            "Working on dataset: X_smoothed_mean_norm_month\n",
            "Working on fold: 1\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 22ms/step - loss: 41.3797 - accuracy: 0.4941 - val_loss: 11.2520 - val_accuracy: 0.6364\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 5.3222 - accuracy: 0.7529 - val_loss: 8.0133 - val_accuracy: 0.5000\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 5.3902 - accuracy: 0.7412 - val_loss: 14.1096 - val_accuracy: 0.7273\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.1232 - accuracy: 0.8941 - val_loss: 15.6849 - val_accuracy: 0.6818\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.6778 - accuracy: 0.9176 - val_loss: 10.7852 - val_accuracy: 0.6364\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Working on fold: 2\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 2s 27ms/step - loss: 23.9669 - accuracy: 0.5176 - val_loss: 15.2598 - val_accuracy: 0.3636\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 11.7404 - accuracy: 0.7294 - val_loss: 8.8192 - val_accuracy: 0.5909\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0465 - accuracy: 0.9294 - val_loss: 9.1877 - val_accuracy: 0.6364\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.7265 - accuracy: 0.9647 - val_loss: 11.5666 - val_accuracy: 0.6818\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.0284 - accuracy: 0.9882 - val_loss: 12.3177 - val_accuracy: 0.6364\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Working on fold: 3\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 22ms/step - loss: 35.9553 - accuracy: 0.6047 - val_loss: 52.3138 - val_accuracy: 0.5909\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 18.4408 - accuracy: 0.7326 - val_loss: 66.6741 - val_accuracy: 0.6818\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 11.5184 - accuracy: 0.8372 - val_loss: 51.5205 - val_accuracy: 0.7273\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 12.5526 - accuracy: 0.8372 - val_loss: 28.4660 - val_accuracy: 0.5000\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 4.5182 - accuracy: 0.9186 - val_loss: 37.3039 - val_accuracy: 0.5909\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Dataset: X_smoothed_mean_norm_month\n",
            "Accuracy: 0.5893314698346145 +/- 0.08317934416473478\n",
            "Sensitivity: 0.5943452144690534 +/- 0.02854351557406235\n",
            "False Positive Rate: 0.3501759678230267 +/- 0.3309075558924957\n",
            "Specificity: 0.6498240321769734 +/- 0.3309075558924957\n",
            "Precision: 0.6681504292646508 +/- 0.09588601144841005\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm_month\n",
            "Accuracy: 0.6087817377125554 +/- 0.024707284922990343\n",
            "Sensitivity: 0.5496908750004725 +/- 0.022459985618095736\n",
            "False Positive Rate: 0.21487825899590607 +/- 0.025739180275967184\n",
            "Specificity: 0.7851217410040939 +/- 0.025739180275967184\n",
            "Precision: 0.5611061738147746 +/- 0.029737200420561147\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_mean_norm\n",
            "Accuracy: 0.5159562077801071 +/- 0.1165314296316961\n",
            "Sensitivity: 0.5244137871072856 +/- 0.05672229597543082\n",
            "False Positive Rate: 0.3939524527759821 +/- 0.30286980063280217\n",
            "Specificity: 0.6060475472240178 +/- 0.30286980063280217\n",
            "Precision: 0.5217549598623767 +/- 0.07099989175088521\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm\n",
            "Accuracy: 0.6583973911017936 +/- 0.06805578421456863\n",
            "Sensitivity: 0.5871742894188714 +/- 0.020542103304235802\n",
            "False Positive Rate: 0.1061732385261797 +/- 0.07248435114464193\n",
            "Specificity: 0.8938267614738202 +/- 0.07248435114464193\n",
            "Precision: 0.6553776792538591 +/- 0.023814390249551697\n",
            "\n",
            "\n",
            "Working on dataset: X_smoothed_median_norm\n",
            "Working on fold: 1\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 22ms/step - loss: 30.5103 - accuracy: 0.5176 - val_loss: 31.7640 - val_accuracy: 0.3636\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 10.0701 - accuracy: 0.8000 - val_loss: 20.5982 - val_accuracy: 0.5455\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 3.0367 - accuracy: 0.9059 - val_loss: 19.4865 - val_accuracy: 0.5000\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 2.7986 - accuracy: 0.8941 - val_loss: 19.7983 - val_accuracy: 0.7273\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.1228 - accuracy: 0.9765 - val_loss: 18.7418 - val_accuracy: 0.7273\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Working on fold: 2\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 3s 28ms/step - loss: 26.6241 - accuracy: 0.4824 - val_loss: 18.2730 - val_accuracy: 0.8182\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 8.3390 - accuracy: 0.8235 - val_loss: 13.8726 - val_accuracy: 0.7727\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 3.4191 - accuracy: 0.9294 - val_loss: 52.9660 - val_accuracy: 0.3182\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 4.6003 - accuracy: 0.8706 - val_loss: 19.5940 - val_accuracy: 0.7273\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.2060 - accuracy: 0.9882 - val_loss: 29.3516 - val_accuracy: 0.7727\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Working on fold: 3\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 21ms/step - loss: 20.9521 - accuracy: 0.5581 - val_loss: 22.4353 - val_accuracy: 0.6364\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 3.4225 - accuracy: 0.8256 - val_loss: 24.4814 - val_accuracy: 0.5909\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.2947 - accuracy: 0.9070 - val_loss: 29.1618 - val_accuracy: 0.6364\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.0421 - accuracy: 0.9884 - val_loss: 21.1498 - val_accuracy: 0.6818\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 2.3703e-07 - accuracy: 1.0000 - val_loss: 20.1069 - val_accuracy: 0.6364\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Dataset: X_smoothed_mean_norm_month\n",
            "Accuracy: 0.5893314698346145 +/- 0.08317934416473478\n",
            "Sensitivity: 0.5943452144690534 +/- 0.02854351557406235\n",
            "False Positive Rate: 0.3501759678230267 +/- 0.3309075558924957\n",
            "Specificity: 0.6498240321769734 +/- 0.3309075558924957\n",
            "Precision: 0.6681504292646508 +/- 0.09588601144841005\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm_month\n",
            "Accuracy: 0.6087817377125554 +/- 0.024707284922990343\n",
            "Sensitivity: 0.5496908750004725 +/- 0.022459985618095736\n",
            "False Positive Rate: 0.21487825899590607 +/- 0.025739180275967184\n",
            "Specificity: 0.7851217410040939 +/- 0.025739180275967184\n",
            "Precision: 0.5611061738147746 +/- 0.029737200420561147\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_mean_norm\n",
            "Accuracy: 0.5159562077801071 +/- 0.1165314296316961\n",
            "Sensitivity: 0.5244137871072856 +/- 0.05672229597543082\n",
            "False Positive Rate: 0.3939524527759821 +/- 0.30286980063280217\n",
            "Specificity: 0.6060475472240178 +/- 0.30286980063280217\n",
            "Precision: 0.5217549598623767 +/- 0.07099989175088521\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm\n",
            "Accuracy: 0.5967854647099929 +/- 0.07387437977059848\n",
            "Sensitivity: 0.5408485391457528 +/- 0.06638688144502103\n",
            "False Positive Rate: 0.17790706026000144 +/- 0.0780662910400039\n",
            "Specificity: 0.8220929397399986 +/- 0.07806629104000391\n",
            "Precision: 0.524346878390996 +/- 0.09264883782655675\n",
            "\n",
            "\n",
            "Working on dataset: X_smoothed_median_norm_month\n",
            "Working on fold: 1\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 26.7900 - accuracy: 0.5176 - val_loss: 1.0789 - val_accuracy: 0.6364\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0694 - accuracy: 0.6824 - val_loss: 1.0791 - val_accuracy: 0.7273\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0744 - accuracy: 0.6706 - val_loss: 1.0643 - val_accuracy: 0.7273\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0611 - accuracy: 0.6706 - val_loss: 1.0496 - val_accuracy: 0.7273\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0479 - accuracy: 0.6706 - val_loss: 1.0365 - val_accuracy: 0.7273\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Working on fold: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 2s 29ms/step - loss: 99.9923 - accuracy: 0.4824 - val_loss: 36.3889 - val_accuracy: 0.6818\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 21.3106 - accuracy: 0.7412 - val_loss: 19.0500 - val_accuracy: 0.6364\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 2.9842 - accuracy: 0.8941 - val_loss: 64.2622 - val_accuracy: 0.1818\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 16.1491 - accuracy: 0.7529 - val_loss: 30.6916 - val_accuracy: 0.8182\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 9.5561 - accuracy: 0.8471 - val_loss: 41.8483 - val_accuracy: 0.4091\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Working on fold: 3\n",
            "Fitting model\n",
            "Epoch 1/5\n",
            "22/22 [==============================] - 1s 22ms/step - loss: 32.6569 - accuracy: 0.4419 - val_loss: 9.9448 - val_accuracy: 0.5000\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 6.4041 - accuracy: 0.7442 - val_loss: 10.2015 - val_accuracy: 0.6364\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 4.7982 - accuracy: 0.7907 - val_loss: 11.9288 - val_accuracy: 0.6364\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.8071 - accuracy: 0.9070 - val_loss: 14.3448 - val_accuracy: 0.4545\n",
            "Predicting test set\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Dataset: X_smoothed_mean_norm_month\n",
            "Accuracy: 0.5893314698346145 +/- 0.08317934416473478\n",
            "Sensitivity: 0.5943452144690534 +/- 0.02854351557406235\n",
            "False Positive Rate: 0.3501759678230267 +/- 0.3309075558924957\n",
            "Specificity: 0.6498240321769734 +/- 0.3309075558924957\n",
            "Precision: 0.6681504292646508 +/- 0.09588601144841005\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm_month\n",
            "Accuracy: 0.47262986256696954 +/- 0.1003897780199082\n",
            "Sensitivity: 0.55306686777275 +/- 0.07686810635873144\n",
            "False Positive Rate: 0.516088486676722 +/- 0.3735401705280771\n",
            "Specificity: 0.48391151332327803 +/- 0.37354017052807703\n",
            "Precision: 0.49363015497767265 +/- 0.19062817030587245\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_mean_norm\n",
            "Accuracy: 0.5159562077801071 +/- 0.1165314296316961\n",
            "Sensitivity: 0.5244137871072856 +/- 0.05672229597543082\n",
            "False Positive Rate: 0.3939524527759821 +/- 0.30286980063280217\n",
            "Specificity: 0.6060475472240178 +/- 0.30286980063280217\n",
            "Precision: 0.5217549598623767 +/- 0.07099989175088521\n",
            "\n",
            "\n",
            "Dataset: X_smoothed_median_norm\n",
            "Accuracy: 0.5967854647099929 +/- 0.07387437977059848\n",
            "Sensitivity: 0.5408485391457528 +/- 0.06638688144502103\n",
            "False Positive Rate: 0.17790706026000144 +/- 0.0780662910400039\n",
            "Specificity: 0.8220929397399986 +/- 0.07806629104000391\n",
            "Precision: 0.524346878390996 +/- 0.09264883782655675\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cnn_dataset_result_ncw = train_and_evaluate(['X_smoothed_mean_norm', 'X_smoothed_mean_norm_month','X_smoothed_median_norm', 'X_smoothed_median_norm_month'], create_cnn_model, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBAgX9CTT5ec"
      },
      "outputs": [],
      "source": [
        "lstm_dataset_results_cw = train_and_evaluate(['X_smoothed_mean_norm', 'X_smoothed_mean_norm_month','X_smoothed_median_norm', 'X_smoothed_median_norm_month'], create_lstm_model, True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrOkYfJUWFVU"
      },
      "outputs": [],
      "source": [
        "lstm_dataset_results_ncw = train_and_evaluate(['X_smoothed_mean_norm', 'X_smoothed_mean_norm_month','X_smoothed_median_norm', 'X_smoothed_median_norm_month'], create_lstm_model, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm2O6QumT5q6"
      },
      "outputs": [],
      "source": [
        "gru_dataset_results_cw = train_and_evaluate(['X_smoothed_mean_norm', 'X_smoothed_mean_norm_month','X_smoothed_median_norm', 'X_smoothed_median_norm_month'], create_gru_model, True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5dVcbIDWX9S"
      },
      "outputs": [],
      "source": [
        "gru_dataset_results_ncw = train_and_evaluate(['X_smoothed_mean_norm', 'X_smoothed_mean_norm_month','X_smoothed_median_norm', 'X_smoothed_median_norm_month'], create_gru_model, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_multitask_cnn_model(input_shape):\n",
        "    # Input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "    \n",
        "    # Convolutional layers\n",
        "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Conv1D(filters=128, kernel_size=3, activation='relu')(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Flatten()(x)\n",
        "    \n",
        "    # Common Dense layer\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    \n",
        "    # Separate output layers for each task\n",
        "    # Output layer for categorical variable (Abnormal)\n",
        "    abnormal_output = Dense(units=3, activation='softmax', name='abnormal_output')(x)  # Assuming 3 classes for the example\n",
        "    \n",
        "    # Output layers for numerical scores (AIMS and optimality)\n",
        "    aims_output = Dense(units=6, activation='softmax', name='aims_output')(x)  # Linear activation for regression\n",
        "    optimality_output = Dense(units=1, name='optimality_output')(x)  # Linear activation for regression\n",
        "    \n",
        "    # Model definition with multiple outputs\n",
        "    model = Model(inputs=inputs, outputs=[abnormal_output, aims_output, optimality_output])\n",
        "    \n",
        "    # Compile model with multiple loss functions and possibly different weights\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'abnormal_output': 'categorical_crossentropy', \n",
        "                        'aims_output': 'mean_squared_error', \n",
        "                        'optimality_output': 'mean_squared_error'},\n",
        "                  loss_weights={'abnormal_output': 0.5, 'aims_output': 0.5, 'optimality_output': 0.5},\n",
        "                  metrics={'abnormal_output': 'accuracy', \n",
        "                           'aims_output': 'accuracy', \n",
        "                           'optimality_output': 'mae'})\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_multitask_model(X_train, y_train_abnormal, y_train_aims, y_train_optimality, X_test, y_test_abnormal, y_test_aims, y_test_optimality):\n",
        "    input_shape = X_train.shape[1:]  # Input shape of the data\n",
        "    \n",
        "    # Create the multitask model\n",
        "    multitask_model = create_multitask_cnn_model(input_shape)\n",
        "    \n",
        "    # Early stopping to prevent overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "    \n",
        "    # Train the model\n",
        "    history = multitask_model.fit(X_train, \n",
        "                                  {'abnormal_output': y_train_abnormal, 'aims_output': y_train_aims, 'optimality_output': y_train_optimality},\n",
        "                                  validation_data=(X_test, \n",
        "                                                   {'abnormal_output': y_test_abnormal, 'aims_output': y_test_aims, 'optimality_output': y_test_optimality}),\n",
        "                                  epochs=100, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
        "    \n",
        "    return multitask_model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Load in dataset from pickle\n",
        "with open('drive/MyDrive/Pickles/X_smoothed_mean_norm_month.pickle', 'rb') as handle:\n",
        "    X_smoothed_mean_norm = pickle.load(handle)\n",
        "\n",
        "X_moothed_mean_norm = np.array(X_smoothed_mean_norm)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train_abnormal, y_test_abnormal, y_train_aims, y_test_aims, y_train_optimality, y_test_optimality = train_test_split(X_smoothed_mean_norm, abnormal_encoded, AIMS, optimality, test_size=0.2, random_state=42)\n",
        "\n",
        "# Put all data into the an array and right format\n",
        "X_train = (np.array(X_train)).astype('float32')\n",
        "X_test = (np.array(X_test)).astype('float32')\n",
        "y_train_abnormal = (np.array(y_train_abnormal)).astype('float32')\n",
        "y_test_abnormal = (np.array(y_test_abnormal)).astype('float32')\n",
        "y_train_aims = (np.array(y_train_aims)).astype('float32')\n",
        "y_test_aims = (np.array(y_test_aims)).astype('float32')\n",
        "y_train_optimality = (np.array(y_train_optimality)).astype('float32')\n",
        "y_test_optimality = (np.array(y_test_optimality)).astype('float32')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DEAFBAA168> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DEAFBAA168> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "4/4 [==============================] - ETA: 0s - loss: 198848.9688 - abnormal_output_loss: 9.5355 - aims_output_loss: 257293.7812 - optimality_output_loss: 140394.6562 - abnormal_output_accuracy: 0.5156 - aims_output_mae: 339.5596 - optimality_output_mae: 285.6335WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DEB0D81678> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DEB0D81678> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "4/4 [==============================] - 9s 2s/step - loss: 198848.9688 - abnormal_output_loss: 9.5355 - aims_output_loss: 257293.7812 - optimality_output_loss: 140394.6562 - abnormal_output_accuracy: 0.5156 - aims_output_mae: 339.5596 - optimality_output_mae: 285.6335 - val_loss: 110275.4688 - val_abnormal_output_loss: 16.2568 - val_aims_output_loss: 117906.8203 - val_optimality_output_loss: 102627.8750 - val_abnormal_output_accuracy: 0.4545 - val_aims_output_mae: 135.2487 - val_optimality_output_mae: 297.0706\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 6s 1s/step - loss: 146266.2812 - abnormal_output_loss: 10.1519 - aims_output_loss: 182519.8438 - optimality_output_loss: 110002.5781 - abnormal_output_accuracy: 0.6797 - aims_output_mae: 261.3097 - optimality_output_mae: 293.0586 - val_loss: 93461.5234 - val_abnormal_output_loss: 24.1760 - val_aims_output_loss: 116342.4922 - val_optimality_output_loss: 70556.3750 - val_abnormal_output_accuracy: 0.4545 - val_aims_output_mae: 283.1516 - val_optimality_output_mae: 191.3405\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 6s 1s/step - loss: 129533.8984 - abnormal_output_loss: 18.6286 - aims_output_loss: 157510.8281 - optimality_output_loss: 101538.3438 - abnormal_output_accuracy: 0.6797 - aims_output_mae: 348.1636 - optimality_output_mae: 222.5030 - val_loss: 101933.5547 - val_abnormal_output_loss: 42.3358 - val_aims_output_loss: 143658.4062 - val_optimality_output_loss: 60166.3516 - val_abnormal_output_accuracy: 0.4545 - val_aims_output_mae: 356.0331 - val_optimality_output_mae: 159.4536\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 113896.2578 - abnormal_output_loss: 22.2932 - aims_output_loss: 143990.9062 - optimality_output_loss: 83779.3125 - abnormal_output_accuracy: 0.6797 - aims_output_mae: 336.0810 - optimality_output_mae: 216.3441 - val_loss: 92708.9062 - val_abnormal_output_loss: 40.8027 - val_aims_output_loss: 122646.1484 - val_optimality_output_loss: 62730.8516 - val_abnormal_output_accuracy: 0.4545 - val_aims_output_mae: 304.3807 - val_optimality_output_mae: 188.6579\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 91778.1875 - abnormal_output_loss: 29.5436 - aims_output_loss: 119911.8281 - optimality_output_loss: 63614.9922 - abnormal_output_accuracy: 0.6797 - aims_output_mae: 290.8105 - optimality_output_mae: 204.8781 - val_loss: 92178.8906 - val_abnormal_output_loss: 76.9728 - val_aims_output_loss: 138815.8438 - val_optimality_output_loss: 45464.9688 - val_abnormal_output_accuracy: 0.4545 - val_aims_output_mae: 336.0943 - val_optimality_output_mae: 163.4500\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 73288.6719 - abnormal_output_loss: 47.1575 - aims_output_loss: 97094.2500 - optimality_output_loss: 49435.9375 - abnormal_output_accuracy: 0.6797 - aims_output_mae: 262.8502 - optimality_output_mae: 168.3337 - val_loss: 81415.5469 - val_abnormal_output_loss: 78.7180 - val_aims_output_loss: 118690.1094 - val_optimality_output_loss: 44062.2539 - val_abnormal_output_accuracy: 0.4242 - val_aims_output_mae: 274.9244 - val_optimality_output_mae: 156.1429\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 53481.6406 - abnormal_output_loss: 51.9810 - aims_output_loss: 74992.7344 - optimality_output_loss: 31918.5742 - abnormal_output_accuracy: 0.5625 - aims_output_mae: 196.7465 - optimality_output_mae: 132.3156 - val_loss: 88756.6562 - val_abnormal_output_loss: 83.9871 - val_aims_output_loss: 143746.7344 - val_optimality_output_loss: 33682.6094 - val_abnormal_output_accuracy: 0.4242 - val_aims_output_mae: 335.1688 - val_optimality_output_mae: 118.2339\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 38167.2188 - abnormal_output_loss: 64.4767 - aims_output_loss: 56592.5547 - optimality_output_loss: 19677.4141 - abnormal_output_accuracy: 0.5312 - aims_output_mae: 199.5939 - optimality_output_mae: 93.4848 - val_loss: 75968.2031 - val_abnormal_output_loss: 117.9406 - val_aims_output_loss: 116465.8516 - val_optimality_output_loss: 35352.6289 - val_abnormal_output_accuracy: 0.2424 - val_aims_output_mae: 253.0976 - val_optimality_output_mae: 126.6038\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 25570.3672 - abnormal_output_loss: 116.4950 - aims_output_loss: 36432.7109 - optimality_output_loss: 14591.5273 - abnormal_output_accuracy: 0.3359 - aims_output_mae: 143.0830 - optimality_output_mae: 84.3256 - val_loss: 100945.6250 - val_abnormal_output_loss: 111.4749 - val_aims_output_loss: 169926.1250 - val_optimality_output_loss: 31853.6621 - val_abnormal_output_accuracy: 0.3939 - val_aims_output_mae: 360.8510 - val_optimality_output_mae: 124.4350\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 17395.4473 - abnormal_output_loss: 121.7113 - aims_output_loss: 25273.3652 - optimality_output_loss: 9395.8184 - abnormal_output_accuracy: 0.3828 - aims_output_mae: 129.7957 - optimality_output_mae: 71.5342 - val_loss: 75960.2656 - val_abnormal_output_loss: 148.4819 - val_aims_output_loss: 119056.3125 - val_optimality_output_loss: 32715.7500 - val_abnormal_output_accuracy: 0.2424 - val_aims_output_mae: 258.4491 - val_optimality_output_mae: 118.5166\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 10738.9121 - abnormal_output_loss: 133.1116 - aims_output_loss: 14359.2041 - optimality_output_loss: 6985.5103 - abnormal_output_accuracy: 0.3281 - aims_output_mae: 96.1882 - optimality_output_mae: 68.5512 - val_loss: 78555.3750 - val_abnormal_output_loss: 123.9313 - val_aims_output_loss: 124571.0703 - val_optimality_output_loss: 32415.7461 - val_abnormal_output_accuracy: 0.3333 - val_aims_output_mae: 275.6846 - val_optimality_output_mae: 127.7508\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 6380.6348 - abnormal_output_loss: 130.8423 - aims_output_loss: 7796.1411 - optimality_output_loss: 4834.2861 - abnormal_output_accuracy: 0.3203 - aims_output_mae: 67.0564 - optimality_output_mae: 59.2834 - val_loss: 85250.1250 - val_abnormal_output_loss: 117.3991 - val_aims_output_loss: 139560.0000 - val_optimality_output_loss: 30822.8379 - val_abnormal_output_accuracy: 0.3636 - val_aims_output_mae: 305.6323 - val_optimality_output_mae: 123.4196\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 3410.1875 - abnormal_output_loss: 99.6908 - aims_output_loss: 4488.8813 - optimality_output_loss: 2231.8032 - abnormal_output_accuracy: 0.3906 - aims_output_mae: 52.6368 - optimality_output_mae: 36.2348 - val_loss: 75424.7812 - val_abnormal_output_loss: 109.7319 - val_aims_output_loss: 120444.1562 - val_optimality_output_loss: 30295.6816 - val_abnormal_output_accuracy: 0.3636 - val_aims_output_mae: 262.6249 - val_optimality_output_mae: 113.8618\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 2013.5984 - abnormal_output_loss: 92.6273 - aims_output_loss: 2368.9370 - optimality_output_loss: 1565.6326 - abnormal_output_accuracy: 0.4062 - aims_output_mae: 38.4272 - optimality_output_mae: 28.6759 - val_loss: 77787.6250 - val_abnormal_output_loss: 89.3385 - val_aims_output_loss: 124463.3281 - val_optimality_output_loss: 31022.5742 - val_abnormal_output_accuracy: 0.4242 - val_aims_output_mae: 278.0158 - val_optimality_output_mae: 119.7701\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1588.9679 - abnormal_output_loss: 73.4713 - aims_output_loss: 1047.6667 - optimality_output_loss: 2056.7976 - abnormal_output_accuracy: 0.4375 - aims_output_mae: 25.9523 - optimality_output_mae: 34.0829 - val_loss: 76871.0391 - val_abnormal_output_loss: 79.0213 - val_aims_output_loss: 123664.5859 - val_optimality_output_loss: 29998.4727 - val_abnormal_output_accuracy: 0.4545 - val_aims_output_mae: 271.3774 - val_optimality_output_mae: 115.6409\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 1036.9690 - abnormal_output_loss: 54.7554 - aims_output_loss: 869.5195 - optimality_output_loss: 1149.6631 - abnormal_output_accuracy: 0.4766 - aims_output_mae: 23.4075 - optimality_output_mae: 26.1288 - val_loss: 79226.7188 - val_abnormal_output_loss: 55.5712 - val_aims_output_loss: 128339.3203 - val_optimality_output_loss: 30058.5488 - val_abnormal_output_accuracy: 0.4848 - val_aims_output_mae: 283.6416 - val_optimality_output_mae: 112.8994\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 744.5199 - abnormal_output_loss: 43.4029 - aims_output_loss: 743.2942 - optimality_output_loss: 702.3427 - abnormal_output_accuracy: 0.5000 - aims_output_mae: 21.1152 - optimality_output_mae: 20.7214 - val_loss: 74536.2734 - val_abnormal_output_loss: 40.7806 - val_aims_output_loss: 119247.7812 - val_optimality_output_loss: 29783.9863 - val_abnormal_output_accuracy: 0.5152 - val_aims_output_mae: 258.4539 - val_optimality_output_mae: 109.1156\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 566.6819 - abnormal_output_loss: 36.1632 - aims_output_loss: 574.9544 - optimality_output_loss: 522.2462 - abnormal_output_accuracy: 0.5156 - aims_output_mae: 19.4942 - optimality_output_mae: 17.9344 - val_loss: 77453.0078 - val_abnormal_output_loss: 32.7664 - val_aims_output_loss: 125373.9844 - val_optimality_output_loss: 29499.2598 - val_abnormal_output_accuracy: 0.5152 - val_aims_output_mae: 276.7735 - val_optimality_output_mae: 110.3766\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 412.1531 - abnormal_output_loss: 32.1431 - aims_output_loss: 326.7545 - optimality_output_loss: 465.4086 - abnormal_output_accuracy: 0.5469 - aims_output_mae: 14.0311 - optimality_output_mae: 17.9137 - val_loss: 76291.4531 - val_abnormal_output_loss: 27.2652 - val_aims_output_loss: 122743.6797 - val_optimality_output_loss: 29811.9688 - val_abnormal_output_accuracy: 0.4848 - val_aims_output_mae: 267.3459 - val_optimality_output_mae: 113.0202\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 344.0615 - abnormal_output_loss: 29.9088 - aims_output_loss: 301.1023 - optimality_output_loss: 357.1119 - abnormal_output_accuracy: 0.5703 - aims_output_mae: 13.9424 - optimality_output_mae: 15.2513 - val_loss: 75604.8438 - val_abnormal_output_loss: 27.5727 - val_aims_output_loss: 121628.5703 - val_optimality_output_loss: 29553.5410 - val_abnormal_output_accuracy: 0.5758 - val_aims_output_mae: 264.3853 - val_optimality_output_mae: 110.6353\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 217.8573 - abnormal_output_loss: 26.1234 - aims_output_loss: 227.0246 - optimality_output_loss: 182.5667 - abnormal_output_accuracy: 0.6484 - aims_output_mae: 11.5920 - optimality_output_mae: 9.9945 - val_loss: 76206.4688 - val_abnormal_output_loss: 28.5171 - val_aims_output_loss: 122453.6953 - val_optimality_output_loss: 29930.7207 - val_abnormal_output_accuracy: 0.5455 - val_aims_output_mae: 265.9298 - val_optimality_output_mae: 113.4942\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 153.4666 - abnormal_output_loss: 24.8290 - aims_output_loss: 123.8768 - optimality_output_loss: 158.2274 - abnormal_output_accuracy: 0.6797 - aims_output_mae: 8.9110 - optimality_output_mae: 9.6546 - val_loss: 76500.4922 - val_abnormal_output_loss: 31.2349 - val_aims_output_loss: 123732.3984 - val_optimality_output_loss: 29237.3535 - val_abnormal_output_accuracy: 0.5455 - val_aims_output_mae: 268.4780 - val_optimality_output_mae: 109.0941\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 136.4453 - abnormal_output_loss: 23.6756 - aims_output_loss: 110.5930 - optimality_output_loss: 138.6220 - abnormal_output_accuracy: 0.6797 - aims_output_mae: 8.1031 - optimality_output_mae: 8.8170 - val_loss: 76955.8750 - val_abnormal_output_loss: 27.0789 - val_aims_output_loss: 124385.5859 - val_optimality_output_loss: 29499.0957 - val_abnormal_output_accuracy: 0.5758 - val_aims_output_mae: 270.6371 - val_optimality_output_mae: 110.9760\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 100.7077 - abnormal_output_loss: 21.7244 - aims_output_loss: 69.0777 - optimality_output_loss: 110.6133 - abnormal_output_accuracy: 0.6562 - aims_output_mae: 6.3732 - optimality_output_mae: 7.4320 - val_loss: 75903.3906 - val_abnormal_output_loss: 23.4157 - val_aims_output_loss: 122175.6484 - val_optimality_output_loss: 29607.7168 - val_abnormal_output_accuracy: 0.5758 - val_aims_output_mae: 263.6877 - val_optimality_output_mae: 111.7926\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 62.0772 - abnormal_output_loss: 19.9758 - aims_output_loss: 43.3758 - optimality_output_loss: 60.8028 - abnormal_output_accuracy: 0.6484 - aims_output_mae: 5.1039 - optimality_output_mae: 5.9822 - val_loss: 76793.3047 - val_abnormal_output_loss: 20.2633 - val_aims_output_loss: 124407.7266 - val_optimality_output_loss: 29158.6113 - val_abnormal_output_accuracy: 0.5758 - val_aims_output_mae: 271.1645 - val_optimality_output_mae: 109.0275\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 53.7137 - abnormal_output_loss: 18.8658 - aims_output_loss: 27.3712 - optimality_output_loss: 61.1904 - abnormal_output_accuracy: 0.6406 - aims_output_mae: 4.1234 - optimality_output_mae: 6.1968 - val_loss: 76399.4375 - val_abnormal_output_loss: 17.5095 - val_aims_output_loss: 123233.4844 - val_optimality_output_loss: 29547.8867 - val_abnormal_output_accuracy: 0.5758 - val_aims_output_mae: 267.4109 - val_optimality_output_mae: 111.5870\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 5s 1s/step - loss: 30.7612 - abnormal_output_loss: 17.4954 - aims_output_loss: 12.8019 - optimality_output_loss: 31.2251 - abnormal_output_accuracy: 0.6328 - aims_output_mae: 2.7293 - optimality_output_mae: 4.0088 - val_loss: 76438.7422 - val_abnormal_output_loss: 17.3650 - val_aims_output_loss: 123261.3750 - val_optimality_output_loss: 29598.7227 - val_abnormal_output_accuracy: 0.6061 - val_aims_output_mae: 268.4485 - val_optimality_output_mae: 111.7170\n",
            "Epoch 27: early stopping\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 76438.7422 - abnormal_output_loss: 17.3650 - aims_output_loss: 123261.3750 - optimality_output_loss: 29598.7227 - abnormal_output_accuracy: 0.6061 - aims_output_mae: 268.4485 - optimality_output_mae: 111.7170\n",
            "Test Loss, Test Accuracy: [76438.7421875, 17.364952087402344, 123261.375, 29598.72265625, 0.6060606241226196, 268.4485168457031, 111.71696472167969]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example call to the training function (assuming data is prepared)\n",
        "multitask_model, history = train_multitask_model(X_train, y_train_abnormal, y_train_aims, y_train_optimality, X_test, y_test_abnormal, y_test_aims, y_test_optimality)\n",
        "\n",
        "# Evaluate the model\n",
        "results = multitask_model.evaluate(X_test, {'abnormal_output': y_test_abnormal, 'aims_output': y_test_aims, 'optimality_output': y_test_optimality}, verbose=1)\n",
        "\n",
        "print(f\"Test Loss, Test Accuracy: {results}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# visualise AIMS distribution\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(AIMS, bins=100)\n",
        "plt.show()\n",
        "\n",
        "# create filtered aims where scores are not 999\n",
        "filtered_aims = [i for i in AIMS if i <10]\n",
        "# visualise filtered AIMS distribution\n",
        "plt.hist(filtered_aims, bins=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the AIMS scores > 6 are invalid. Filter out the invalid data points from AIMS, optimality, abnormal_encoded and X_smoothed_mean_norm_month\n",
        "filtered_aims = [i for i in AIMS if i <6]\n",
        "filtered_optimality = [optimality[i] for i in range(len(AIMS)) if AIMS[i] <6]\n",
        "filtered_abnormal_encoded = [abnormal_encoded[i] for i in range(len(AIMS)) if AIMS[i] <6]\n",
        "filtered_X_smoothed_mean_norm_month = [X_smoothed_mean_norm[i] for i in range(len(AIMS)) if AIMS[i] <6]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "# one-hot encode filtered_aims\n",
        "filtered_aims_encoded = []\n",
        "\n",
        "# Use the sklearn OneHotEncoder to one-hot encode the data\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Create the OneHotEncoder object\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# One-hot encode the data\n",
        "filtered_aims_encoded = onehot_encoder.fit_transform(np.array(filtered_aims).reshape(-1, 1))\n",
        "\n",
        "print(filtered_aims_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train_abnormal, y_test_abnormal, y_train_aims, y_test_aims, y_train_optimality, y_test_optimality = train_test_split(filtered_X_smoothed_mean_norm_month, filtered_abnormal_encoded, filtered_aims_encoded, filtered_optimality, test_size=0.2, random_state=42)\n",
        "\n",
        "# Put all data into the an array and right format\n",
        "X_train = (np.array(X_train)).astype('float32')\n",
        "X_test = (np.array(X_test)).astype('float32')\n",
        "y_train_abnormal = (np.array(y_train_abnormal)).astype('float32')\n",
        "y_test_abnormal = (np.array(y_test_abnormal)).astype('float32')\n",
        "y_train_aims = (np.array(y_train_aims)).astype('float32')\n",
        "y_test_aims = (np.array(y_test_aims)).astype('float32')\n",
        "y_train_optimality = (np.array(y_train_optimality)).astype('float32')\n",
        "y_test_optimality = (np.array(y_test_optimality)).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DEC5A47288> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DEC5A47288> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "4/4 [==============================] - ETA: 0s - loss: 122077.9922 - abnormal_output_loss: 35.9493 - aims_output_loss: 0.2478 - optimality_output_loss: 244119.7656 - abnormal_output_accuracy: 0.5464 - aims_output_accuracy: 0.1237 - optimality_output_mae: 396.0086WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DEAE7838B8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DEAE7838B8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "4/4 [==============================] - 5s 1s/step - loss: 122077.9922 - abnormal_output_loss: 35.9493 - aims_output_loss: 0.2478 - optimality_output_loss: 244119.7656 - abnormal_output_accuracy: 0.5464 - aims_output_accuracy: 0.1237 - optimality_output_mae: 396.0086 - val_loss: 67656.0000 - val_abnormal_output_loss: 6.2297 - val_aims_output_loss: 0.2800 - val_optimality_output_loss: 135305.4844 - val_abnormal_output_accuracy: 0.6000 - val_aims_output_accuracy: 0.1600 - val_optimality_output_mae: 226.6853\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 4s 941ms/step - loss: 69345.9141 - abnormal_output_loss: 8.9723 - aims_output_loss: 0.2955 - optimality_output_loss: 138682.5781 - abnormal_output_accuracy: 0.4433 - aims_output_accuracy: 0.1134 - optimality_output_mae: 241.9805 - val_loss: 54721.4336 - val_abnormal_output_loss: 33.8115 - val_aims_output_loss: 0.2800 - val_optimality_output_loss: 109408.7734 - val_abnormal_output_accuracy: 0.2400 - val_aims_output_accuracy: 0.1600 - val_optimality_output_mae: 265.1590\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 4s 904ms/step - loss: 44846.0039 - abnormal_output_loss: 28.2503 - aims_output_loss: 0.2955 - optimality_output_loss: 89663.4609 - abnormal_output_accuracy: 0.4021 - aims_output_accuracy: 0.1134 - optimality_output_mae: 250.7678 - val_loss: 51534.0352 - val_abnormal_output_loss: 32.6829 - val_aims_output_loss: 0.2741 - val_optimality_output_loss: 103035.1172 - val_abnormal_output_accuracy: 0.2400 - val_aims_output_accuracy: 0.1600 - val_optimality_output_mae: 253.6587\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 4s 916ms/step - loss: 42214.8359 - abnormal_output_loss: 23.7636 - aims_output_loss: 0.2846 - optimality_output_loss: 84405.6250 - abnormal_output_accuracy: 0.4227 - aims_output_accuracy: 0.1443 - optimality_output_mae: 219.6272 - val_loss: 51896.1836 - val_abnormal_output_loss: 25.7236 - val_aims_output_loss: 0.2467 - val_optimality_output_loss: 103766.3984 - val_abnormal_output_accuracy: 0.2800 - val_aims_output_accuracy: 0.2400 - val_optimality_output_mae: 210.9971\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 4s 945ms/step - loss: 37731.2539 - abnormal_output_loss: 20.9074 - aims_output_loss: 0.2769 - optimality_output_loss: 75441.3203 - abnormal_output_accuracy: 0.4330 - aims_output_accuracy: 0.1134 - optimality_output_mae: 195.9736 - val_loss: 40849.5039 - val_abnormal_output_loss: 28.7794 - val_aims_output_loss: 0.2784 - val_optimality_output_loss: 81669.9531 - val_abnormal_output_accuracy: 0.3600 - val_aims_output_accuracy: 0.1200 - val_optimality_output_mae: 212.5016\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 4s 943ms/step - loss: 24461.9941 - abnormal_output_loss: 16.1554 - aims_output_loss: 0.2705 - optimality_output_loss: 48907.5625 - abnormal_output_accuracy: 0.4227 - aims_output_accuracy: 0.1031 - optimality_output_mae: 168.9785 - val_loss: 38793.7812 - val_abnormal_output_loss: 13.4324 - val_aims_output_loss: 0.2261 - val_optimality_output_loss: 77573.9062 - val_abnormal_output_accuracy: 0.2400 - val_aims_output_accuracy: 0.2800 - val_optimality_output_mae: 175.5940\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 4s 905ms/step - loss: 31718.9258 - abnormal_output_loss: 6.7860 - aims_output_loss: 0.2405 - optimality_output_loss: 63430.8242 - abnormal_output_accuracy: 0.4021 - aims_output_accuracy: 0.1340 - optimality_output_mae: 146.1255 - val_loss: 39446.3203 - val_abnormal_output_loss: 9.2994 - val_aims_output_loss: 0.2214 - val_optimality_output_loss: 78883.1172 - val_abnormal_output_accuracy: 0.3200 - val_aims_output_accuracy: 0.2800 - val_optimality_output_mae: 160.8747\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 4s 899ms/step - loss: 20675.3086 - abnormal_output_loss: 6.2412 - aims_output_loss: 0.2388 - optimality_output_loss: 41344.1367 - abnormal_output_accuracy: 0.4742 - aims_output_accuracy: 0.1546 - optimality_output_mae: 125.4281 - val_loss: 31866.1934 - val_abnormal_output_loss: 15.1721 - val_aims_output_loss: 0.2821 - val_optimality_output_loss: 63716.9336 - val_abnormal_output_accuracy: 0.4800 - val_aims_output_accuracy: 0.1200 - val_optimality_output_mae: 178.9680\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 4s 914ms/step - loss: 17550.2227 - abnormal_output_loss: 12.7560 - aims_output_loss: 0.2992 - optimality_output_loss: 35087.3906 - abnormal_output_accuracy: 0.5876 - aims_output_accuracy: 0.0928 - optimality_output_mae: 128.0893 - val_loss: 30447.8809 - val_abnormal_output_loss: 13.7295 - val_aims_output_loss: 0.2655 - val_optimality_output_loss: 60881.7656 - val_abnormal_output_accuracy: 0.4400 - val_aims_output_accuracy: 0.1200 - val_optimality_output_mae: 167.5962\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 4s 945ms/step - loss: 11498.5918 - abnormal_output_loss: 9.9715 - aims_output_loss: 0.2768 - optimality_output_loss: 22986.9336 - abnormal_output_accuracy: 0.5773 - aims_output_accuracy: 0.1134 - optimality_output_mae: 100.5627 - val_loss: 33691.1016 - val_abnormal_output_loss: 8.6505 - val_aims_output_loss: 0.2201 - val_optimality_output_loss: 67373.3359 - val_abnormal_output_accuracy: 0.4000 - val_aims_output_accuracy: 0.1600 - val_optimality_output_mae: 147.4520\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 4s 864ms/step - loss: 10978.9502 - abnormal_output_loss: 5.1193 - aims_output_loss: 0.2251 - optimality_output_loss: 21952.5547 - abnormal_output_accuracy: 0.5052 - aims_output_accuracy: 0.1856 - optimality_output_mae: 88.7308 - val_loss: 30673.3906 - val_abnormal_output_loss: 11.5526 - val_aims_output_loss: 0.2264 - val_optimality_output_loss: 61335.0039 - val_abnormal_output_accuracy: 0.3200 - val_aims_output_accuracy: 0.1200 - val_optimality_output_mae: 150.2888\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 4s 864ms/step - loss: 7303.3389 - abnormal_output_loss: 9.4837 - aims_output_loss: 0.2527 - optimality_output_loss: 14596.9424 - abnormal_output_accuracy: 0.5979 - aims_output_accuracy: 0.1546 - optimality_output_mae: 75.4280 - val_loss: 31894.2891 - val_abnormal_output_loss: 10.6663 - val_aims_output_loss: 0.2200 - val_optimality_output_loss: 63777.6914 - val_abnormal_output_accuracy: 0.3600 - val_aims_output_accuracy: 0.1600 - val_optimality_output_mae: 152.9299\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 4s 918ms/step - loss: 4530.7114 - abnormal_output_loss: 8.5105 - aims_output_loss: 0.2414 - optimality_output_loss: 9052.6699 - abnormal_output_accuracy: 0.5361 - aims_output_accuracy: 0.1753 - optimality_output_mae: 60.3282 - val_loss: 33287.2539 - val_abnormal_output_loss: 9.5472 - val_aims_output_loss: 0.2152 - val_optimality_output_loss: 66564.7422 - val_abnormal_output_accuracy: 0.3600 - val_aims_output_accuracy: 0.1600 - val_optimality_output_mae: 153.0234\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 4s 940ms/step - loss: 4891.4297 - abnormal_output_loss: 8.3328 - aims_output_loss: 0.2400 - optimality_output_loss: 9774.2871 - abnormal_output_accuracy: 0.5670 - aims_output_accuracy: 0.1443 - optimality_output_mae: 62.3079 - val_loss: 36245.2812 - val_abnormal_output_loss: 10.6984 - val_aims_output_loss: 0.2075 - val_optimality_output_loss: 72479.6562 - val_abnormal_output_accuracy: 0.4000 - val_aims_output_accuracy: 0.1600 - val_optimality_output_mae: 156.6544\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 4s 908ms/step - loss: 3674.1824 - abnormal_output_loss: 5.6055 - aims_output_loss: 0.2353 - optimality_output_loss: 7342.5244 - abnormal_output_accuracy: 0.5876 - aims_output_accuracy: 0.1134 - optimality_output_mae: 54.5877 - val_loss: 37079.0547 - val_abnormal_output_loss: 7.6177 - val_aims_output_loss: 0.2074 - val_optimality_output_loss: 74150.2812 - val_abnormal_output_accuracy: 0.3600 - val_aims_output_accuracy: 0.3600 - val_optimality_output_mae: 154.2618\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 4s 877ms/step - loss: 3359.9102 - abnormal_output_loss: 3.8128 - aims_output_loss: 0.2187 - optimality_output_loss: 6715.7886 - abnormal_output_accuracy: 0.5567 - aims_output_accuracy: 0.1649 - optimality_output_mae: 49.2476 - val_loss: 36681.8320 - val_abnormal_output_loss: 17.0694 - val_aims_output_loss: 0.2150 - val_optimality_output_loss: 73346.3828 - val_abnormal_output_accuracy: 0.2800 - val_aims_output_accuracy: 0.3200 - val_optimality_output_mae: 153.0594\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 4s 874ms/step - loss: 2180.5938 - abnormal_output_loss: 8.6853 - aims_output_loss: 0.2492 - optimality_output_loss: 4352.2529 - abnormal_output_accuracy: 0.4742 - aims_output_accuracy: 0.1546 - optimality_output_mae: 40.6371 - val_loss: 35770.1953 - val_abnormal_output_loss: 15.5843 - val_aims_output_loss: 0.2150 - val_optimality_output_loss: 71524.5938 - val_abnormal_output_accuracy: 0.2800 - val_aims_output_accuracy: 0.3200 - val_optimality_output_mae: 151.0926\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 4s 863ms/step - loss: 1396.3340 - abnormal_output_loss: 6.5653 - aims_output_loss: 0.2477 - optimality_output_loss: 2785.8547 - abnormal_output_accuracy: 0.5464 - aims_output_accuracy: 0.1546 - optimality_output_mae: 31.9132 - val_loss: 34944.6484 - val_abnormal_output_loss: 12.9464 - val_aims_output_loss: 0.2160 - val_optimality_output_loss: 69876.1328 - val_abnormal_output_accuracy: 0.3600 - val_aims_output_accuracy: 0.3200 - val_optimality_output_mae: 149.5454\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 4s 866ms/step - loss: 1012.1628 - abnormal_output_loss: 6.6433 - aims_output_loss: 0.2562 - optimality_output_loss: 2017.4263 - abnormal_output_accuracy: 0.5773 - aims_output_accuracy: 0.1443 - optimality_output_mae: 29.4484 - val_loss: 34049.3086 - val_abnormal_output_loss: 20.4386 - val_aims_output_loss: 0.2759 - val_optimality_output_loss: 68077.9062 - val_abnormal_output_accuracy: 0.4000 - val_aims_output_accuracy: 0.2000 - val_optimality_output_mae: 146.7966\n",
            "Epoch 19: early stopping\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 34049.3086 - abnormal_output_loss: 20.4386 - aims_output_loss: 0.2759 - optimality_output_loss: 68077.9062 - abnormal_output_accuracy: 0.4000 - aims_output_accuracy: 0.2000 - optimality_output_mae: 146.7966\n",
            "Test Loss, Test Accuracy: [34049.30859375, 20.4386043548584, 0.27593758702278137, 68077.90625, 0.4000000059604645, 0.20000000298023224, 146.79661560058594]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example call to the training function (assuming data is prepared)\n",
        "multitask_model, history = train_multitask_model(X_train, y_train_abnormal, y_train_aims, y_train_optimality, X_test, y_test_abnormal, y_test_aims, y_test_optimality)\n",
        "\n",
        "# Evaluate the model\n",
        "results = multitask_model.evaluate(X_test, {'abnormal_output': y_test_abnormal, 'aims_output': y_test_aims, 'optimality_output': y_test_optimality}, verbose=1)\n",
        "\n",
        "print(f\"Test Loss, Test Accuracy: {results}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
